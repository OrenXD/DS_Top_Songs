{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import html\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import time\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (10, 6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Note:<br>\n",
    "## If you want to build the DataFrame by your own and download the relevant files to achive this task then begin from here.<br> \n",
    "## If you want to run the code when you already using existed DataFrame and downloaded files, Please skip to Phase 2 - Learning Algorithm & Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Data Acquisition & Data Cleaning & Data Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Load and clean the data\n",
    "The following cells perform 2 things:\n",
    "* load the csv file which contain the dataframe of spotify songs\n",
    "* replace all of the unnecessary punctuation in each title and artist for the further work with scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify=pd.read_csv(\"./data/spotify.csv\")\n",
    "df_spotify=df_spotify.drop('Unnamed: 0', 1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to clean the titles and artists string for the search in metrolyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDf(df):\n",
    "    for i in range(0,len(df)):\n",
    "        df['Title'][i]=df['Title'][i].replace('MotorSport','motor sport').replace('PILLOWTALK','pillow talk').replace('Back To You','Back 2 you')\n",
    "        \n",
    "        list=df['Title'][i].split(' (feat. ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split('- From ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split(' feat. ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split(' (with ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split(' (Original ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split(' (From ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split(' (Fifty ')\n",
    "        df['Title'][i]=list[0]\n",
    "        \n",
    "        df['Title'][i]=df['Title'][i].replace(\"Wanna\",\"want to\").replace('\\n','').replace(';','').replace(\"'\",\"\").replace(',','').replace('/ ','').replace('- ','').replace('-','').replace('é','e').replace('?','').replace('\"','').replace(\"!\",\"\").replace(\"in'\",\"ing\").replace(\" and \",\" \").replace(\"’\",\"\")\n",
    "        df['Title'][i]=df['Title'][i].replace('(','').replace(')','').replace('.','')\n",
    "        \n",
    "        df['Artist'][i]=df['Artist'][i].replace('P!nk','Pink').replace('NERD','nerd the neptunes').replace('ZAYN','zayn malik').replace('Axwell /\\ Ingrosso','Axwell Ingrosso').replace('Ayo & Teo','Ayo Teo')\n",
    "        df['Artist'][i]=df['Artist'][i].replace('é','e').replace('í','i').replace('.','').replace('\\n','').replace(' + ','-').replace('.','').replace('ó','o').replace('$','s').replace(\"'\",\"\").replace(\"!\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "cleanDf(df_spotify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Artist</th>\n",
       "      <th>id</th>\n",
       "      <th>artist_genres</th>\n",
       "      <th>Year</th>\n",
       "      <th>is_top100</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Love Yourself</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>3hB5DgAiMAQ4DzYbsMq1IT</td>\n",
       "      <td>['canadian pop', 'pop', 'post-teen pop']</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.83500</td>\n",
       "      <td>0.609</td>\n",
       "      <td>233720</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>-9.828</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4380</td>\n",
       "      <td>100.418</td>\n",
       "      <td>4</td>\n",
       "      <td>0.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Sorry</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>69bp2EbF7Q2rqc5N3ylezZ</td>\n",
       "      <td>['canadian pop', 'pop', 'post-teen pop']</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.07970</td>\n",
       "      <td>0.654</td>\n",
       "      <td>200787</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2990</td>\n",
       "      <td>-3.669</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>99.945</td>\n",
       "      <td>4</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>One Dance</td>\n",
       "      <td>Drake</td>\n",
       "      <td>1xznGGDReH1oQq0xzbwXa3</td>\n",
       "      <td>['canadian hip hop', 'canadian pop', 'hip hop'...</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00784</td>\n",
       "      <td>0.791</td>\n",
       "      <td>173987</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.004230</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3510</td>\n",
       "      <td>-5.886</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0532</td>\n",
       "      <td>103.989</td>\n",
       "      <td>4</td>\n",
       "      <td>0.371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Work</td>\n",
       "      <td>Rihanna</td>\n",
       "      <td>14WWzenpaEgQZlqPq2nk4v</td>\n",
       "      <td>['barbadian pop', 'dance pop', 'pop', 'post-te...</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.07520</td>\n",
       "      <td>0.725</td>\n",
       "      <td>219320</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0919</td>\n",
       "      <td>-6.238</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0946</td>\n",
       "      <td>91.974</td>\n",
       "      <td>4</td>\n",
       "      <td>0.558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Stressed Out</td>\n",
       "      <td>Twenty One Pilots</td>\n",
       "      <td>3CRDbSIZ4r5MsZ0YwxuEkn</td>\n",
       "      <td>['modern rock', 'rock']</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04620</td>\n",
       "      <td>0.734</td>\n",
       "      <td>202333</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0602</td>\n",
       "      <td>-5.677</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>169.977</td>\n",
       "      <td>4</td>\n",
       "      <td>0.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>Tiimmy Turner</td>\n",
       "      <td>Desiigner</td>\n",
       "      <td>0zMxL4BTjSqCsUtfdlcL8G</td>\n",
       "      <td>['pop rap', 'rap', 'southern hip hop', 'trap',...</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.16800</td>\n",
       "      <td>0.602</td>\n",
       "      <td>239853</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0781</td>\n",
       "      <td>-3.021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0405</td>\n",
       "      <td>122.812</td>\n",
       "      <td>4</td>\n",
       "      <td>0.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>See You Again</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>7wqSzGeodspE3V6RBD5W8L</td>\n",
       "      <td>['hip hop', 'pittsburgh rap', 'pop rap', 'rap'...</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.36900</td>\n",
       "      <td>0.689</td>\n",
       "      <td>229526</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>-7.503</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0815</td>\n",
       "      <td>80.025</td>\n",
       "      <td>4</td>\n",
       "      <td>0.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>One Direction</td>\n",
       "      <td>3NLnwwAQbbFKcEcV8hDItk</td>\n",
       "      <td>['boy band', 'dance pop', 'pop', 'post-teen po...</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05980</td>\n",
       "      <td>0.647</td>\n",
       "      <td>230333</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>-5.231</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0762</td>\n",
       "      <td>99.933</td>\n",
       "      <td>4</td>\n",
       "      <td>0.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>0FE9t6xYkqWXU2ahLh6D8X</td>\n",
       "      <td>['pop', 'uk pop']</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>0.58100</td>\n",
       "      <td>0.825</td>\n",
       "      <td>233713</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>-3.183</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0802</td>\n",
       "      <td>95.977</td>\n",
       "      <td>4</td>\n",
       "      <td>0.931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>Despacito Remix</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>5CtI0qwDJkDQGwXD1H1cLb</td>\n",
       "      <td>['latin', 'latin pop', 'puerto rican pop', 'tr...</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22900</td>\n",
       "      <td>0.694</td>\n",
       "      <td>228827</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>-4.328</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>88.931</td>\n",
       "      <td>4</td>\n",
       "      <td>0.813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Title             Artist                      id  \\\n",
       "0     Love Yourself      Justin Bieber  3hB5DgAiMAQ4DzYbsMq1IT   \n",
       "1             Sorry      Justin Bieber  69bp2EbF7Q2rqc5N3ylezZ   \n",
       "2         One Dance              Drake  1xznGGDReH1oQq0xzbwXa3   \n",
       "3              Work            Rihanna  14WWzenpaEgQZlqPq2nk4v   \n",
       "4      Stressed Out  Twenty One Pilots  3CRDbSIZ4r5MsZ0YwxuEkn   \n",
       "..              ...                ...                     ...   \n",
       "95    Tiimmy Turner          Desiigner  0zMxL4BTjSqCsUtfdlcL8G   \n",
       "96    See You Again        Wiz Khalifa  7wqSzGeodspE3V6RBD5W8L   \n",
       "97          Perfect      One Direction  3NLnwwAQbbFKcEcV8hDItk   \n",
       "98     Shape of You         Ed Sheeran  0FE9t6xYkqWXU2ahLh6D8X   \n",
       "99  Despacito Remix         Luis Fonsi  5CtI0qwDJkDQGwXD1H1cLb   \n",
       "\n",
       "                                        artist_genres  Year  is_top100  \\\n",
       "0            ['canadian pop', 'pop', 'post-teen pop']  2016          1   \n",
       "1            ['canadian pop', 'pop', 'post-teen pop']  2016          1   \n",
       "2   ['canadian hip hop', 'canadian pop', 'hip hop'...  2016          1   \n",
       "3   ['barbadian pop', 'dance pop', 'pop', 'post-te...  2016          1   \n",
       "4                             ['modern rock', 'rock']  2016          1   \n",
       "..                                                ...   ...        ...   \n",
       "95  ['pop rap', 'rap', 'southern hip hop', 'trap',...  2016          1   \n",
       "96  ['hip hop', 'pittsburgh rap', 'pop rap', 'rap'...  2016          1   \n",
       "97  ['boy band', 'dance pop', 'pop', 'post-teen po...  2016          1   \n",
       "98                                  ['pop', 'uk pop']  2017          1   \n",
       "99  ['latin', 'latin pop', 'puerto rican pop', 'tr...  2017          1   \n",
       "\n",
       "    acousticness  danceability  duration_ms  energy  instrumentalness  key  \\\n",
       "0        0.83500         0.609       233720   0.378          0.000000    4   \n",
       "1        0.07970         0.654       200787   0.760          0.000000    0   \n",
       "2        0.00784         0.791       173987   0.619          0.004230    1   \n",
       "3        0.07520         0.725       219320   0.534          0.000000   11   \n",
       "4        0.04620         0.734       202333   0.637          0.000023    4   \n",
       "..           ...           ...          ...     ...               ...  ...   \n",
       "95       0.16800         0.602       239853   0.723          0.000000    6   \n",
       "96       0.36900         0.689       229526   0.481          0.000001   10   \n",
       "97       0.05980         0.647       230333   0.823          0.000000    2   \n",
       "98       0.58100         0.825       233713   0.652          0.000000    1   \n",
       "99       0.22900         0.694       228827   0.815          0.000000    2   \n",
       "\n",
       "    liveness  loudness  mode  speechiness    tempo  time_signature  valence  \n",
       "0     0.2800    -9.828     1       0.4380  100.418               4    0.515  \n",
       "1     0.2990    -3.669     0       0.0450   99.945               4    0.410  \n",
       "2     0.3510    -5.886     1       0.0532  103.989               4    0.371  \n",
       "3     0.0919    -6.238     1       0.0946   91.974               4    0.558  \n",
       "4     0.0602    -5.677     0       0.1410  169.977               4    0.648  \n",
       "..       ...       ...   ...          ...      ...             ...      ...  \n",
       "95    0.0781    -3.021     1       0.0405  122.812               4    0.293  \n",
       "96    0.0649    -7.503     1       0.0815   80.025               4    0.283  \n",
       "97    0.1190    -5.231     1       0.0762   99.933               4    0.396  \n",
       "98    0.0931    -3.183     0       0.0802   95.977               4    0.931  \n",
       "99    0.0924    -4.328     1       0.1200   88.931               4    0.813  \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spotify[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the amount of rows in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_spotify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Load and store lyrics data\n",
    "\n",
    "* clean the text from unnecessary punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to create the lyrics_urls list to store each song's lyrics url on metrolyrics\n",
    "The following function perform 2 things:\n",
    "* create list of urls from metrolyrics for each song \n",
    "* replace all of the unnecessary strings from the title of each song\n",
    "* handle with cases of several artists of a song. split them and take the first one to build with it a url\n",
    "* replace all of the spaces to '-' for the url\n",
    "* clean the text from unnecessary punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_lyrics_urls(df):\n",
    "    #creating the list of urls for each song's lyrics\n",
    "    lyrics_urls=[]\n",
    "    for i in range (0,len(df)):\n",
    "        #the case of & in songs:\n",
    "        name=df['Title'][i].replace(\"& \",\"\").replace(\"'\",\"\")\n",
    "        name=name.replace(\" \",\"-\")\n",
    "\n",
    "        list=df['Artist'][i].split(',')\n",
    "        #we will always take the first artist\n",
    "        artist_str=list[0]\n",
    "        list=artist_str.split(' & ')\n",
    "        artist_str=list[0]\n",
    "        list=artist_str.split(' and ')\n",
    "        artist_str=list[0]\n",
    "        list=artist_str.split('featuring')\n",
    "        artist_str=list[0]\n",
    "\n",
    "        url='https://www.metrolyrics.com/'+name+'-lyrics-'+artist_str.replace('\"','').replace(\" \",\"-\")+'.html'\n",
    "        #the case of \"Maroon-5-.html\"\n",
    "        url=url.replace(\"-.\",\".\") \n",
    "\n",
    "        lyrics_urls.append(url)      \n",
    "    return lyrics_urls    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=create_lyrics_urls(df_spotify)\n",
    "len(urls) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the nltk package for the further work with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords, movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to delete the stopwords from the lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_mystopwords(string):\n",
    "    tokens = string.split(\" \")\n",
    "    tokens_filtered = [word for word in tokens if not word in stop_words]\n",
    "    return (\" \").join(tokens_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following cell pefrom 4 things:\n",
    "* scrap the metrolyrics\n",
    "* load the lyrics of each song\n",
    "* clean the text from digits, stopwords, and unnecessary punctuation\n",
    "* check the amount of words in each song "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "#the part of creation of the lyrics_texts dictionary which contain the lyrics of each song in top 2016,2017,2018\n",
    "\n",
    "all_words_count=[]\n",
    "\n",
    "def find_lyrics_txt(df):\n",
    "    lyrics_texts=[]\n",
    "    time.sleep(7)\n",
    "    for i in range (0,len(df)):\n",
    "        url = urls[i]\n",
    "        response = requests.get(url)\n",
    "        data = response.text\n",
    "        soup = bs(data,'html.parser')\n",
    "        couplets = soup.findAll(\"p\",{\"class\":\"verse\"})\n",
    "        temp = \"\"\n",
    "        for p in couplets:\n",
    "            temp = str(temp) + str(p.text)\n",
    "            temp=temp+\"\\n\"\n",
    "        #cleaning the text from unnecessary punctuation\n",
    "        temp=re.sub(r'[^(a-zA-Z)\\s]','', temp)\n",
    "        temp=temp.replace('[','').replace(']','').replace('(','').replace(')','').replace('\"','').replace(\"'\",\"\")\n",
    "        #remove all of the digits \n",
    "        temp=''.join(j for j in temp if not j.isdigit())\n",
    "        #count the number of words in each song\n",
    "        all_words_count.append(len(temp))\n",
    "        #remove all of the stopwords\n",
    "        temp = remove_mystopwords(temp)\n",
    "        #for the further work with text we would like to lowercase all of the words in text   \n",
    "        lyrics_texts.append(temp.lower())\n",
    "        lyrics_texts[i]=lyrics_texts[i].replace('\\n ','\\n')  \n",
    "    return lyrics_texts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics=find_lyrics_txt(df_spotify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_without_empty_strings = []\n",
    "for string in lyrics:\n",
    "    if (string != \"\"):\n",
    "        lyrics_without_empty_strings.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we want to check the amount of songs with zero words (the algorithm of finding the url for metrolyrics didn't work for them) so we can delete them in the dataframe in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120, 123, 135, 162, 241, 246, 279, 298, 300, 301, 302, 308, 311, 315, 316, 317, 318, 319, 322, 329, 333, 338, 341, 352, 364, 376, 378, 383, 387, 388, 390, 392, 397, 404, 406, 410, 414, 415, 417, 430, 432, 435, 442, 444, 445, 451, 453, 455, 458, 466, 473, 476, 480, 481, 482, 483, 486, 487, 498, 502, 510, 519, 521, 523, 531, 532, 537, 540, 544, 554, 561, 563, 568, 579]\n"
     ]
    }
   ],
   "source": [
    "res_list = [] \n",
    "count=0\n",
    "for i in range(0, len(all_words_count)) : \n",
    "    if all_words_count[i] == 0 : \n",
    "        res_list.append(i)    \n",
    "print(res_list)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we create a function to add a column to our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_col(df,lst,colName):\n",
    "    df[colName] = lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add to our dataframe the 'amount of words in song' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_col(df_spotify,all_words_count,'amount of words in song')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning: In the next cell we drop every row which value of amout of words in song is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range (0,len(df_spotify)):\n",
    "    if (df_spotify['amount of words in song'][i]) == 0:\n",
    "        df_spotify['amount of words in song'][i]=pd.np.NaN\n",
    "        \n",
    "df_spotify=df_spotify.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the amount of rows in a new dataframe without nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_spotify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Work with lyrics text\n",
    "The following cells pefrom 3 things:\n",
    "* create a list of negative words and find how many times do they appear in each song\n",
    "* create a list of positive words and find how many times do they appear in each song\n",
    "* find a total count of repeated words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a text as positive or negative using textblob package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the polarity value for each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "# The sentiment function of textblob returns two properties, polarity, and subjectivity.\n",
    "# Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement\n",
    "\n",
    "def pos_neg_neutral(lyrics_texts):\n",
    "    blobs=[]\n",
    "    for i in range (0,len(lyrics_texts)):\n",
    "        blob=TextBlob(lyrics_texts[i])\n",
    "        pnn=blob.sentiment.polarity\n",
    "        definition=''\n",
    "        if -0.3 < pnn < 0.3:\n",
    "            definition='neutral'\n",
    "        if pnn  <= -0.3:\n",
    "            definition='negative'\n",
    "        if pnn >= 0.3:\n",
    "            definition='positive'\n",
    "        blobs.append(definition)    \n",
    "    return blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs_polarity=pos_neg_neutral(lyrics_without_empty_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blobs_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'negative', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'negative', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "print(blobs_polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we will check hom many negative,neutral,positive songs do we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': 50, 'neg': 7, 'neutral': 455}\n"
     ]
    }
   ],
   "source": [
    "blobs_res={\"pos\": 0, \"neg\": 0, \"neutral\": 0}\n",
    "for i in range (0,len(blobs_polarity)):\n",
    "    if blobs_polarity[i]==\"positive\":\n",
    "        blobs_res[\"pos\"]+=1\n",
    "    if blobs_polarity[i]==\"negative\":\n",
    "        blobs_res[\"neg\"]+=1\n",
    "    if blobs_polarity[i]==\"neutral\":\n",
    "        blobs_res[\"neutral\"]+=1\n",
    "print(blobs_res)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_col(df_spotify,blobs_polarity,'blobs polarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we will tokenize (convert the whole lyrics text to words) the lyrics of each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lyrics=[]\n",
    "\n",
    "for i in range (0,len(df_spotify)):\n",
    "    tokens=word_tokenize(lyrics_without_empty_strings[i])\n",
    "    tokenized_lyrics.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'times', 'rain', 'parade', 'and', 'clubs', 'get', 'using', 'name', 'you', 'think', 'broke', 'heart', 'oh', 'girl', 'goodness', 'sake', 'you', 'think', 'im', 'crying', 'well', 'i', 'aint', 'and', 'i', 'didnt', 'wan', 'na', 'write', 'song', 'cause', 'i', 'didnt', 'want', 'anyone', 'thinking', 'i', 'still', 'care', 'i', 'dont', 'still', 'hit', 'phone', 'up', 'and', 'baby', 'i', 'movin', 'on', 'and', 'i', 'think', 'somethin', 'i', 'dont', 'wan', 'na', 'hold', 'back', 'maybe', 'know', 'that', 'my', 'mama', 'dont', 'like', 'likes', 'everyone', 'and', 'i', 'never', 'like', 'admit', 'i', 'wrong', 'and', 'ive', 'caught', 'job', 'didnt', 'see', 'whats', 'going', 'on', 'but', 'i', 'know', 'im', 'better', 'sleeping', 'own', 'cause', 'like', 'way', 'look', 'much', 'oh', 'baby', 'go', 'love', 'yourself', 'and', 'think', 'im', 'still', 'holdin', 'somethin', 'you', 'go', 'love', 'yourself', 'and', 'told', 'hated', 'friends', 'the', 'problem', 'them', 'and', 'every', 'time', 'told', 'opinion', 'wrong', 'and', 'tried', 'make', 'forget', 'i', 'came', 'from', 'and', 'i', 'didnt', 'wan', 'na', 'write', 'song', 'cause', 'i', 'didnt', 'want', 'anyone', 'thinking', 'i', 'still', 'care', 'i', 'dont', 'still', 'hit', 'phone', 'up', 'and', 'baby', 'i', 'movin', 'on', 'and', 'i', 'think', 'somethin', 'i', 'dont', 'wan', 'na', 'hold', 'back', 'maybe', 'know', 'that', 'my', 'mama', 'dont', 'like', 'likes', 'everyone', 'and', 'i', 'never', 'like', 'admit', 'i', 'wrong', 'and', 'ive', 'caught', 'job', 'didnt', 'see', 'whats', 'going', 'on', 'but', 'i', 'know', 'im', 'better', 'sleeping', 'own', 'cause', 'like', 'way', 'look', 'much', 'oh', 'baby', 'go', 'love', 'yourself', 'and', 'think', 'im', 'still', 'holdin', 'somethin', 'you', 'go', 'love', 'yourself', 'for', 'times', 'made', 'feel', 'small', 'i', 'fell', 'love', 'i', 'feel', 'nothin', 'all', 'had', 'never', 'felt', 'low', 'i', 'vulnerable', 'was', 'i', 'fool', 'let', 'break', 'walls', 'cause', 'like', 'way', 'look', 'much', 'oh', 'baby', 'go', 'love', 'yourself', 'and', 'think', 'im', 'still', 'holdin', 'somethin', 'you', 'go', 'love', 'yourself', 'cause', 'like', 'way', 'look', 'much', 'oh', 'baby', 'go', 'love', 'yourself', 'and', 'think', 'im', 'still', 'holdin', 'somethin', 'you', 'go', 'love', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "print(str(tokenized_lyrics[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350, 600, 525, 450, 525, 200, 200, 400, 850, 425, 375, 300, 275, 825, 675, 175, 150, 450, 800, 200, 400, 875, 150, 900, 225, 950, 300, 325, 575, 550, 275, 175, 675, 350, 450, 200, 300, 350, 375, 425, 200, 425, 775, 125, 325, 275, 175, 325, 300, 575, 400, 1050, 275, 625, 200, 175, 1875, 450, 1025, 325, 275, 75, 250, 450, 275, 350, 100, 400, 225, 725, 325, 250, 1050, 525, 475, 275, 1000, 175, 525, 375, 175, 950, 150, 2375, 175, 1250, 625, 375, 275, 425, 125, 675, 100, 500, 275, 525, 200, 800, 125, 50, 675, 475, 550, 700, 425, 275, 500, 825, 375, 2450, 325, 200, 650, 600, 200, 275, 475, 450, 25, 425, 725, 375, 200, 400, 325, 650, 575, 350, 125, 575, 725, 925, 425, 225, 275, 250, 250, 275, 600, 450, 175, 575, 700, 850, 0, 400, 225, 1275, 450, 500, 250, 400, 1325, 625, 450, 475, 500, 625, 175, 300, 300, 275, 350, 375, 375, 650, 675, 225, 350, 1250, 175, 0, 525, 225, 675, 475, 400, 275, 875, 450, 175, 125, 175, 300, 325, 250, 325, 425, 125, 400, 350, 0, 175, 175, 525, 400, 125, 250, 250, 275, 125, 425, 475, 600, 575, 675, 1200, 700, 575, 625, 475, 300, 625, 200, 400, 225, 550, 275, 200, 775, 250, 700, 1575, 525, 150, 450, 550, 175, 150, 275, 275, 100, 300, 575, 975, 200, 50, 575, 250, 225, 1250, 750, 375, 500, 725, 525, 100, 375, 50, 500, 125, 475, 850, 675, 575, 400, 600, 175, 450, 500, 275, 125, 250, 475, 125, 700, 300, 600, 100, 750, 450, 150, 1175, 575, 275, 350, 300, 1275, 400, 50, 400, 375, 25, 0, 100, 300, 750, 725, 0, 500, 50, 325, 50, 100, 100, 100, 425, 75, 1050, 1325, 425, 250, 675, 625, 1500, 250, 100, 225, 450, 425, 50, 475, 100, 500, 200, 475, 975, 925, 275, 250, 525, 625, 750, 425, 175, 50, 375, 750, 450, 175, 400, 500, 400, 525, 250, 50, 225, 500, 100, 125, 500, 300, 525, 525, 150, 225, 750, 200, 350, 300, 50, 325, 200, 700, 400, 900, 725, 125, 525, 1025, 275, 50, 350, 650, 225, 325, 475, 200, 525, 300, 1250, 350, 50, 25, 525, 325, 75, 225, 875, 150, 375, 250, 150, 450, 325, 975, 25, 375, 225, 475, 225, 475, 525, 175, 200, 125, 225, 325, 475, 250, 150, 550, 450, 200, 0, 200, 250, 525, 200, 125, 625, 225, 750, 150, 250, 875, 50, 0, 625, 325, 425, 100, 0, 150, 325, 150, 300, 125, 625, 375, 425, 750, 675, 350, 350, 175, 100, 425, 300, 275, 750, 225, 200, 200, 50, 1275, 275, 800, 275, 600, 150, 325, 0, 175, 200, 625, 975, 350, 550, 700, 300, 100, 450, 400, 400, 1325, 725, 575, 250, 650, 200, 125, 325, 275, 400, 950, 200, 50, 375, 325, 275, 250, 1150, 400, 0, 225, 300, 1100, 625, 25, 250, 75, 275, 350, 725, 775, 350, 600, 325, 275, 425, 475, 175, 300, 150, 1150, 225, 250, 300, 450, 525, 350]\n"
     ]
    }
   ],
   "source": [
    "#here we would like to count how many positive words can we see in each song lyrics\n",
    "\n",
    "count_pos=[]\n",
    "\n",
    "keywords_pos=[]\n",
    "\n",
    "url='https://www.enchantedlearning.com/wordlist/positivewords.shtml'\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "soup = bs(data,'html.parser')\n",
    "words = soup.findAll(\"div\",{\"class\":\"wordlist-section\"})\n",
    "\n",
    "for div in words:\n",
    "    divs=soup.findAll(\"div\",{\"class\":\"wordlist-item\"})\n",
    "    for div in divs:\n",
    "        keywords_pos.append(div.text)\n",
    "\n",
    "for i in range (0,len(df_spotify)):\n",
    "    temp=[]\n",
    "    for word in keywords_pos:\n",
    "        #here we create a temp list to store all the words from keywords_neg which appear in lyrics \n",
    "        temp+=re.findall(word, str(lyrics_without_empty_strings[i]))\n",
    "    #now we'd want to know how many times these words appeared in text\n",
    "    count_pos.append(len(temp))\n",
    "print(count_pos)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[648, 1104, 384, 816, 336, 600, 312, 288, 480, 840, 552, 1032, 504, 456, 1008, 240, 72, 624, 1032, 336, 264, 672, 672, 600, 600, 360, 456, 360, 288, 552, 960, 168, 336, 312, 456, 288, 456, 1272, 456, 1320, 408, 672, 576, 336, 960, 600, 624, 672, 240, 456, 216, 1656, 360, 192, 216, 288, 312, 360, 672, 552, 408, 192, 168, 408, 360, 1104, 576, 1056, 240, 312, 504, 168, 960, 504, 456, 480, 1056, 96, 600, 432, 456, 120, 168, 2976, 168, 1008, 840, 648, 216, 336, 168, 528, 72, 168, 2064, 1080, 432, 432, 192, 360, 288, 408, 120, 1968, 840, 240, 456, 792, 552, 840, 672, 408, 1152, 288, 144, 432, 1032, 360, 408, 336, 672, 264, 312, 816, 360, 600, 624, 480, 288, 288, 624, 576, 144, 912, 312, 768, 96, 480, 1152, 720, 624, 1488, 192, 480, 192, 120, 576, 504, 216, 384, 312, 264, 1296, 888, 336, 576, 456, 552, 432, 312, 168, 1728, 312, 216, 240, 384, 864, 288, 144, 1008, 144, 360, 648, 168, 528, 288, 288, 480, 624, 576, 168, 240, 96, 168, 360, 264, 384, 672, 96, 528, 984, 0, 1608, 504, 336, 624, 96, 312, 912, 552, 264, 312, 240, 816, 1104, 1032, 528, 696, 480, 768, 864, 480, 528, 96, 120, 264, 384, 384, 336, 1200, 240, 840, 240, 432, 312, 720, 1080, 120, 264, 480, 264, 408, 576, 744, 552, 168, 144, 336, 864, 432, 1008, 264, 528, 168, 672, 264, 432, 24, 312, 432, 120, 816, 312, 312, 1488, 192, 144, 144, 72, 624, 24, 192, 264, 1056, 480, 456, 672, 528, 264, 384, 720, 936, 168, 624, 744, 504, 408, 336, 528, 312, 288, 720, 192, 144, 96, 216, 672, 432, 192, 456, 432, 192, 264, 0, 936, 192, 264, 96, 672, 360, 792, 288, 1080, 648, 648, 192, 48, 264, 336, 528, 168, 504, 960, 48, 120, 840, 288, 528, 456, 552, 1032, 144, 672, 0, 648, 360, 552, 840, 384, 192, 840, 288, 240, 72, 288, 72, 312, 1056, 216, 0, 600, 312, 1272, 360, 312, 408, 1080, 360, 480, 528, 72, 744, 216, 936, 456, 888, 1008, 288, 1224, 960, 576, 48, 552, 648, 192, 720, 456, 408, 480, 840, 384, 744, 72, 144, 432, 672, 0, 360, 744, 48, 624, 576, 96, 168, 336, 696, 48, 120, 408, 264, 504, 624, 312, 816, 432, 408, 576, 312, 360, 648, 312, 408, 72, 168, 192, 624, 480, 576, 144, 480, 384, 312, 2064, 168, 240, 456, 192, 240, 480, 384, 504, 0, 216, 360, 600, 168, 288, 528, 672, 384, 216, 672, 576, 576, 216, 408, 120, 696, 408, 432, 504, 336, 624, 456, 168, 816, 360, 744, 288, 288, 480, 120, 0, 336, 480, 576, 1320, 240, 288, 216, 72, 672, 72, 1920, 696, 768, 624, 552, 1104, 504, 432, 168, 240, 336, 528, 480, 432, 264, 240, 192, 552, 768, 264, 456, 264, 840, 120, 768, 672, 72, 384, 288, 96, 528, 984, 1296, 192, 480, 432, 264, 312, 528, 168, 840, 96, 672, 480, 144, 72, 120, 360, 48]\n"
     ]
    }
   ],
   "source": [
    "#here we would like to count how many negative words can we see in each song lyrics\n",
    "\n",
    "count_neg=[]\n",
    "\n",
    "keywords_neg=[]\n",
    "\n",
    "url='https://www.enchantedlearning.com/wordlist/negativewords.shtml'\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "soup = bs(data,'html.parser')\n",
    "words = soup.findAll(\"div\",{\"class\":\"wordlist-section\"})\n",
    "\n",
    "for div in words:\n",
    "    divs=soup.findAll(\"div\",{\"class\":\"wordlist-item\"})\n",
    "    for div in divs:\n",
    "        keywords_neg.append(div.text)\n",
    "        \n",
    "for i in range (0,len(df_spotify)):\n",
    "    temp=[]\n",
    "    for word in keywords_neg:\n",
    "        #here we create a temp list to store all the words from keywords_neg which appear in lyrics \n",
    "        temp+=re.findall(word, str(lyrics_without_empty_strings[i]))\n",
    "    #now we'd want to know how many times these words appeared in text\n",
    "    count_neg.append(len(temp))\n",
    "print(count_neg)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 24, 9, 0, 0, 0, 0, 0, 14, 2, 1, 0, 0, 8, 8, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 28, 0, 17, 0, 25, 6, 0, 4, 10, 0, 3, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 5, 12, 1, 0, 11, 13, 6, 16, 6, 3, 0, 0, 11, 6, 0, 1, 5, 1, 4, 0, 7, 11, 0, 2, 0, 1, 0, 10, 0, 0, 0, 0, 1, 0, 10, 10, 16, 6, 2, 3, 0, 0, 11, 41, 1, 0, 0, 1, 1, 51, 0, 50, 0, 1, 1, 8, 0, 5, 2, 19, 7, 4, 1, 3, 6, 13, 0, 0, 22, 8, 0, 8, 1, 0, 1, 0, 3, 0, 23, 0, 1, 0, 1, 1, 1, 2, 0, 0, 2, 7, 25, 0, 0, 2, 23, 70, 0, 10, 13, 0, 0, 21, 1, 13, 0, 16, 20, 2, 4, 1, 25, 4, 1, 3, 3, 6, 0, 0, 0, 20, 9, 23, 3, 7, 0, 4, 21, 0, 0, 0, 0, 0, 0, 18, 2, 27, 1, 0, 16, 25, 0, 0, 3, 0, 1, 13, 7, 8, 0, 16, 1, 10, 2, 1, 3, 0, 1, 1, 0, 3, 0, 2, 2, 28, 2, 6, 0, 1, 23, 1, 44, 15, 0, 0, 14, 3, 1, 0, 20, 3, 4, 0, 17, 7, 27, 0, 4, 3, 0, 1, 24, 0, 22, 2, 46, 21, 0, 16, 0, 44, 0, 21, 7, 0, 0, 0, 1, 1, 0, 0, 18, 4, 15, 4, 0, 0, 0, 20, 0, 1, 1, 1, 0, 6, 0, 18, 0, 0, 5, 0, 0, 5, 1, 0, 30, 9, 0, 1, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 11, 8, 0, 2, 1, 0, 6, 0, 0, 0, 0, 17, 10, 2, 36, 0, 0, 5, 22, 24, 0, 0, 0, 17, 6, 0, 8, 0, 19, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 41, 1, 1, 0, 16, 0, 0, 33, 0, 0, 0, 5, 2, 1, 25, 1, 8, 3, 1, 0, 2, 10, 1, 0, 7, 1, 0, 0, 12, 7, 22, 4, 0, 2, 0, 0, 0, 0, 0, 8, 2, 0, 0, 0, 0, 3, 0, 0, 0, 0, 7, 21, 0, 0, 0, 15, 0, 0, 5, 0, 1, 0, 0, 0, 0, 14, 1, 13, 10, 4, 2, 0, 7, 1, 0, 1, 2, 0, 2, 0, 0, 0, 0, 5, 0, 0, 14, 36, 0, 0, 2, 0, 5, 0, 2, 2, 0, 0, 2, 0, 10, 4, 0, 0, 16, 0, 4, 34, 5, 1, 0, 0, 2, 31, 33, 23, 21, 3, 52, 13, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 9, 23, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 12, 0, 1, 1, 12, 2, 1, 0, 2, 0, 13, 1, 0, 0, 0, 0, 1, 0, 2, 3, 0, 24, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "count_swear=[]\n",
    "\n",
    "keywords_swear=[]\n",
    "\n",
    "url='https://en.wiktionary.org/wiki/Category:English_swear_words'\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "soup = bs(data,'html.parser')\n",
    "div_class = soup.findAll(\"div\",{\"class\":\"mw-category-group\"})\n",
    "\n",
    "for div in div_class:\n",
    "    uls=div.find_all('ul')\n",
    "    for ul in uls:\n",
    "        lis=ul.find_all('li')\n",
    "        for ls in lis:\n",
    "            links=ls.find_all('a')\n",
    "            for link in links:\n",
    "                keywords_swear.append(link.text)\n",
    "                \n",
    "keywords_swear.pop(0)         \n",
    "\n",
    "for k in range(0,len(keywords_swear)):\n",
    "    keywords_swear[k] =keywords_swear[k].lower() \n",
    "\n",
    "# print(keywords_swear)\n",
    "\n",
    "for i in range (0,len(df_spotify)):\n",
    "    temp=[]\n",
    "    for word in keywords_swear:\n",
    "        #here we create a temp list to store all the words from keywords_neg which appear in lyrics \n",
    "        temp+=re.findall(word, str(lyrics_without_empty_strings[i]))\n",
    "    #now we'd want to know how many times these words appeared in text\n",
    "    count_swear.append(len(temp))\n",
    "print(count_swear)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_col(df_spotify,count_neg,'amount of negative words')\n",
    "append_col(df_spotify,count_pos,'amount of positive words')\n",
    "append_col(df_spotify,count_swear,'amount of swear words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we'd like to find the total count of repeated words and the most common words in each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "repeated_words=[]\n",
    "most_common_words=[]\n",
    "\n",
    "reg = re.compile('\\S{4,}')\n",
    "\n",
    "\n",
    "for i in range (0,len(df_spotify)):\n",
    "    c = Counter(ma.group() for ma in reg.finditer(lyrics_without_empty_strings[i]))\n",
    "    repeated_words.append(sum(c.values()))\n",
    "    most_common_words.append([k for k,v in c.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[179, 117, 184, 307, 265, 362, 142, 150, 252, 196, 173, 187, 118, 167, 200, 234, 60, 215, 314, 159, 163, 142, 171, 221, 103, 171, 88, 109, 208, 439, 182, 203, 182, 258, 166, 228, 252, 229, 187, 133, 195, 156, 93, 111, 170, 159, 120, 259, 151, 216, 204, 289, 152, 221, 135, 149, 347, 219, 164, 179, 163, 106, 129, 136, 198, 172, 99, 266, 112, 175, 122, 158, 210, 228, 199, 204, 269, 88, 183, 185, 165, 174, 122, 351, 30, 151, 204, 285, 178, 310, 179, 134, 110, 157, 205, 347, 146, 222, 267, 329, 212, 215, 169, 552, 196, 144, 185, 240, 149, 351, 200, 202, 211, 221, 124, 214, 319, 219, 143, 210, 296, 116, 153, 295, 120, 206, 243, 135, 151, 257, 348, 312, 215, 187, 153, 221, 154, 204, 310, 126, 120, 232, 285, 252, 125, 149, 323, 351, 157, 277, 223, 163, 180, 250, 249, 317, 142, 205, 107, 121, 271, 277, 258, 218, 158, 238, 216, 73, 151, 150, 168, 223, 245, 251, 134, 177, 117, 242, 403, 162, 159, 108, 168, 94, 109, 273, 174, 218, 190, 125, 159, 111, 157, 151, 127, 144, 190, 223, 269, 422, 160, 201, 338, 276, 246, 218, 224, 193, 204, 112, 185, 193, 212, 98, 149, 228, 235, 192, 178, 196, 331, 263, 329, 258, 127, 126, 472, 113, 173, 192, 253, 162, 133, 172, 372, 457, 218, 146, 194, 351, 150, 295, 327, 117, 296, 259, 183, 188, 160, 263, 111, 440, 158, 360, 232, 155, 157, 168, 123, 175, 115, 267, 273, 317, 228, 321, 128, 194, 126, 311, 147, 475, 149, 243, 238, 181, 139, 155, 125, 112, 185, 209, 284, 105, 115, 164, 472, 170, 125, 185, 80, 81, 119, 21, 137, 123, 134, 78, 174, 219, 168, 48, 146, 371, 353, 50, 34, 148, 109, 180, 86, 105, 161, 86, 146, 280, 169, 331, 73, 132, 198, 197, 156, 83, 192, 135, 278, 203, 140, 156, 155, 314, 144, 58, 108, 163, 109, 186, 89, 65, 199, 93, 279, 221, 108, 142, 221, 197, 158, 252, 118, 140, 103, 205, 142, 218, 408, 119, 224, 292, 103, 55, 194, 172, 196, 191, 214, 212, 219, 152, 308, 110, 168, 106, 97, 200, 98, 95, 127, 93, 104, 187, 96, 111, 200, 146, 116, 82, 222, 134, 118, 139, 345, 177, 90, 123, 116, 181, 112, 110, 96, 197, 124, 68, 125, 83, 77, 255, 124, 228, 330, 90, 302, 88, 121, 172, 210, 59, 238, 240, 229, 60, 214, 146, 121, 100, 123, 145, 242, 329, 172, 151, 165, 151, 104, 85, 65, 211, 191, 157, 191, 137, 242, 175, 65, 142, 247, 140, 137, 238, 154, 181, 110, 264, 183, 344, 565, 283, 221, 163, 169, 236, 176, 211, 187, 137, 125, 126, 259, 181, 154, 84, 104, 263, 158, 423, 210, 210, 296, 140, 124, 185, 129, 75, 216, 246, 89, 296, 175, 39, 140, 177, 271, 222, 231, 245, 162, 127, 315, 100, 183, 139, 216, 133, 83, 207, 120, 124, 168, 204, 157, 108]\n"
     ]
    }
   ],
   "source": [
    "print(repeated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'think', 'still', 'like', 'yourself', 'didnt', 'cause', 'dont', 'baby', 'somethin', 'wanna', 'know', 'look', 'much', 'holdin', 'never', 'wrong', 'times', 'write', 'song', 'want', 'anyone', 'thinking', 'care', 'phone', 'movin', 'hold', 'back', 'maybe', 'that', 'mama', 'likes', 'everyone', 'admit', 'caught', 'whats', 'going', 'better', 'sleeping', 'told', 'feel', 'rain', 'parade', 'clubs', 'using', 'name', 'broke', 'heart', 'girl', 'goodness', 'sake', 'crying', 'well', 'aint', 'hated', 'friends', 'problem', 'them', 'every', 'time', 'opinion', 'tried', 'make', 'forget', 'came', 'from', 'made', 'small', 'fell', 'nothin', 'felt', 'vulnerable', 'fool', 'break', 'walls']\n"
     ]
    }
   ],
   "source": [
    "print(most_common_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_col(df_spotify,repeated_words,'amount of repeated words')\n",
    "append_col(df_spotify,most_common_words,'most common words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Artist</th>\n",
       "      <th>id</th>\n",
       "      <th>artist_genres</th>\n",
       "      <th>Year</th>\n",
       "      <th>is_top100</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>...</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "      <th>amount of words in song</th>\n",
       "      <th>blobs polarity</th>\n",
       "      <th>amount of negative words</th>\n",
       "      <th>amount of positive words</th>\n",
       "      <th>amount of swear words</th>\n",
       "      <th>amount of repeated words</th>\n",
       "      <th>most common words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Love Yourself</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>3hB5DgAiMAQ4DzYbsMq1IT</td>\n",
       "      <td>['canadian pop', 'pop', 'post-teen pop']</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.83500</td>\n",
       "      <td>0.609</td>\n",
       "      <td>233720</td>\n",
       "      <td>0.378</td>\n",
       "      <td>...</td>\n",
       "      <td>100.418</td>\n",
       "      <td>4</td>\n",
       "      <td>0.515</td>\n",
       "      <td>2057.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>648</td>\n",
       "      <td>350</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>[love, think, still, like, yourself, didnt, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Sorry</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>69bp2EbF7Q2rqc5N3ylezZ</td>\n",
       "      <td>['canadian pop', 'pop', 'post-teen pop']</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.07970</td>\n",
       "      <td>0.654</td>\n",
       "      <td>200787</td>\n",
       "      <td>0.760</td>\n",
       "      <td>...</td>\n",
       "      <td>99.945</td>\n",
       "      <td>4</td>\n",
       "      <td>0.410</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1104</td>\n",
       "      <td>600</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>[sorry, yeah, late, know, down, cause, missing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>One Dance</td>\n",
       "      <td>Drake</td>\n",
       "      <td>1xznGGDReH1oQq0xzbwXa3</td>\n",
       "      <td>['canadian hip hop', 'canadian pop', 'hip hop'...</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00784</td>\n",
       "      <td>0.791</td>\n",
       "      <td>173987</td>\n",
       "      <td>0.619</td>\n",
       "      <td>...</td>\n",
       "      <td>103.989</td>\n",
       "      <td>4</td>\n",
       "      <td>0.371</td>\n",
       "      <td>1773.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>384</td>\n",
       "      <td>525</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>[back, time, where, need, wine, dance, henness...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Work</td>\n",
       "      <td>Rihanna</td>\n",
       "      <td>14WWzenpaEgQZlqPq2nk4v</td>\n",
       "      <td>['barbadian pop', 'dance pop', 'pop', 'post-te...</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.07520</td>\n",
       "      <td>0.725</td>\n",
       "      <td>219320</td>\n",
       "      <td>0.534</td>\n",
       "      <td>...</td>\n",
       "      <td>91.974</td>\n",
       "      <td>4</td>\n",
       "      <td>0.558</td>\n",
       "      <td>2704.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>816</td>\n",
       "      <td>450</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>[work, dirt, learn, hurt, done, mmmmm, turn, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Stressed Out</td>\n",
       "      <td>Twenty One Pilots</td>\n",
       "      <td>3CRDbSIZ4r5MsZ0YwxuEkn</td>\n",
       "      <td>['modern rock', 'rock']</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04620</td>\n",
       "      <td>0.734</td>\n",
       "      <td>202333</td>\n",
       "      <td>0.637</td>\n",
       "      <td>...</td>\n",
       "      <td>169.977</td>\n",
       "      <td>4</td>\n",
       "      <td>0.648</td>\n",
       "      <td>2494.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>336</td>\n",
       "      <td>525</td>\n",
       "      <td>0</td>\n",
       "      <td>265</td>\n",
       "      <td>[used, wish, play, pretend, sang, time, names,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Title             Artist                      id  \\\n",
       "0  Love Yourself      Justin Bieber  3hB5DgAiMAQ4DzYbsMq1IT   \n",
       "1          Sorry      Justin Bieber  69bp2EbF7Q2rqc5N3ylezZ   \n",
       "2      One Dance              Drake  1xznGGDReH1oQq0xzbwXa3   \n",
       "3           Work            Rihanna  14WWzenpaEgQZlqPq2nk4v   \n",
       "4   Stressed Out  Twenty One Pilots  3CRDbSIZ4r5MsZ0YwxuEkn   \n",
       "\n",
       "                                       artist_genres  Year  is_top100  \\\n",
       "0           ['canadian pop', 'pop', 'post-teen pop']  2016          1   \n",
       "1           ['canadian pop', 'pop', 'post-teen pop']  2016          1   \n",
       "2  ['canadian hip hop', 'canadian pop', 'hip hop'...  2016          1   \n",
       "3  ['barbadian pop', 'dance pop', 'pop', 'post-te...  2016          1   \n",
       "4                            ['modern rock', 'rock']  2016          1   \n",
       "\n",
       "   acousticness  danceability  duration_ms  energy  ...    tempo  \\\n",
       "0       0.83500         0.609       233720   0.378  ...  100.418   \n",
       "1       0.07970         0.654       200787   0.760  ...   99.945   \n",
       "2       0.00784         0.791       173987   0.619  ...  103.989   \n",
       "3       0.07520         0.725       219320   0.534  ...   91.974   \n",
       "4       0.04620         0.734       202333   0.637  ...  169.977   \n",
       "\n",
       "   time_signature  valence  amount of words in song  blobs polarity  \\\n",
       "0               4    0.515                   2057.0         neutral   \n",
       "1               4    0.410                   1479.0        negative   \n",
       "2               4    0.371                   1773.0         neutral   \n",
       "3               4    0.558                   2704.0         neutral   \n",
       "4               4    0.648                   2494.0         neutral   \n",
       "\n",
       "   amount of negative words  amount of positive words  amount of swear words  \\\n",
       "0                       648                       350                      0   \n",
       "1                      1104                       600                      0   \n",
       "2                       384                       525                      0   \n",
       "3                       816                       450                      1   \n",
       "4                       336                       525                      0   \n",
       "\n",
       "   amount of repeated words                                  most common words  \n",
       "0                       179  [love, think, still, like, yourself, didnt, ca...  \n",
       "1                       117  [sorry, yeah, late, know, down, cause, missing...  \n",
       "2                       184  [back, time, where, need, wine, dance, henness...  \n",
       "3                       307  [work, dirt, learn, hurt, done, mmmmm, turn, n...  \n",
       "4                       265  [used, wish, play, pretend, sang, time, names,...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spotify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Learning Algoorithm & Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
