{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import html\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import time\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (10, 6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition & Data Cleaning & Data Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Load and clean the data\n",
    "The following cells perform 2 things:\n",
    "* load the csv file which contain the dataframe of spotify songs\n",
    "* replace all of the unnecessary punctuation in each title and artist for the further work with scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify=pd.read_csv(\"./data/spotify.csv\")\n",
    "df_spotify=df_spotify.drop('Unnamed: 0', 1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to clean the titles and artists string for the search in metrolyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDf(df):\n",
    "    for i in range(0,len(df)):\n",
    "        df['Title'][i]=df['Title'][i].replace('MotorSport','motor sport').replace('PILLOWTALK','pillow talk').replace('Back To You','Back 2 you')\n",
    "        \n",
    "        list=df['Title'][i].split(' (feat. ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split('- From ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split(' feat. ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split(' (with ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split(' (Original ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split(' (From ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split(' (Fifty ')\n",
    "        df['Title'][i]=list[0]\n",
    "        \n",
    "        df['Title'][i]=df['Title'][i].replace(\"Wanna\",\"want to\").replace('\\n','').replace(';','').replace(\"'\",\"\").replace(',','').replace('/ ','').replace('- ','').replace('-','').replace('é','e').replace('?','').replace('\"','').replace(\"!\",\"\").replace(\"in'\",\"ing\").replace(\" and \",\" \").replace(\"’\",\"\")\n",
    "        df['Title'][i]=df['Title'][i].replace('(','').replace(')','').replace('.','')\n",
    "        \n",
    "        df['Artist'][i]=df['Artist'][i].replace('P!nk','Pink').replace('NERD','nerd the neptunes').replace('ZAYN','zayn malik').replace('Axwell /\\ Ingrosso','Axwell Ingrosso').replace('Ayo & Teo','Ayo Teo')\n",
    "        df['Artist'][i]=df['Artist'][i].replace('é','e').replace('í','i').replace('.','').replace('\\n','').replace(' + ','-').replace('.','').replace('ó','o').replace('$','s').replace(\"'\",\"\").replace(\"!\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "cleanDf(df_spotify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we create a function to add a column to our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_col(df,lst,colName):\n",
    "    df[colName] = lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the amount of rows in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "587"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_spotify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Load and store lyrics data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to create the lyrics_urls list to store each song's lyrics url on metrolyrics\n",
    "The following function perform 5 things:\n",
    "* create list of urls from metrolyrics for each song \n",
    "* replace all of the unnecessary strings from the title of each song\n",
    "* handle with cases of several artists of a song. split them and take the first one to build with it a url\n",
    "* replace all of the spaces to '-' for the url\n",
    "* clean the text from unnecessary punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_lyrics_urls(df):\n",
    "    #creating the list of urls for each song's lyrics\n",
    "    lyrics_urls=[]\n",
    "    for i in range (0,len(df)):\n",
    "        #the case of & in songs:\n",
    "        name=df['Title'][i].replace(\"& \",\"\").replace(\"'\",\"\")\n",
    "        name=name.replace(\" \",\"-\")\n",
    "\n",
    "        list=df['Artist'][i].split(',')\n",
    "        #we will always take the first artist\n",
    "        artist_str=list[0]\n",
    "        list=artist_str.split(' & ')\n",
    "        artist_str=list[0]\n",
    "        list=artist_str.split(' and ')\n",
    "        artist_str=list[0]\n",
    "        list=artist_str.split('featuring')\n",
    "        artist_str=list[0]\n",
    "\n",
    "        url='https://www.metrolyrics.com/'+name+'-lyrics-'+artist_str.replace('\"','').replace(\" \",\"-\")+'.html'\n",
    "        #the case of \"Maroon-5-.html\"\n",
    "        url=url.replace(\"-.\",\".\") \n",
    "\n",
    "        lyrics_urls.append(url)      \n",
    "    return lyrics_urls    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "587"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=create_lyrics_urls(df_spotify)\n",
    "len(urls) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the nltk package for the further work with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords, movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to delete the stopwords from the lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_mystopwords(string):\n",
    "    tokens = string.split(\" \")\n",
    "    tokens_filtered = [word for word in tokens if not word in stop_words]\n",
    "    return (\" \").join(tokens_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to count the stopwords in each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_stopwords(string):\n",
    "    count=0\n",
    "    tokens = string.split(\" \")\n",
    "    for word in tokens:\n",
    "        if word in stop_words:\n",
    "            count+=1\n",
    "    return count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following cell pefrom 5 things:\n",
    "\n",
    "* scrap the metrolyrics\n",
    "* load the lyrics of each song\n",
    "* check the amount of words in each song \n",
    "* check the amount of stopwords in each song \n",
    "* clean the text from digits, stopwords, and unnecessary punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the part of creation of the lyrics_texts dictionary which contain the lyrics of each song in top 2016,2017,2018\n",
    "\n",
    "all_words_count=[]\n",
    "stopwords_count=[]\n",
    "\n",
    "def find_lyrics_txt(df):\n",
    "    lyrics_texts=[]\n",
    "    time.sleep(7)\n",
    "    for i in range (0,len(df)):\n",
    "        url = urls[i]\n",
    "        response = requests.get(url)\n",
    "        data = response.text\n",
    "        soup = bs(data,'html.parser')\n",
    "        couplets = soup.findAll(\"p\",{\"class\":\"verse\"})\n",
    "        temp = \"\"\n",
    "        for p in couplets:\n",
    "            temp = str(temp) + str(p.text)\n",
    "            temp=temp+\"\\n\"\n",
    "        #cleaning the text from unnecessary punctuation\n",
    "        temp=re.sub(r'[^(a-zA-Z)\\s]','', temp)\n",
    "        temp=temp.replace('[','').replace(']','').replace('(','').replace(')','').replace('\"','').replace(\"'\",\"\")\n",
    "        #remove all of the digits \n",
    "        temp=''.join(j for j in temp if not j.isdigit())\n",
    "        #count the number of words in each song\n",
    "        all_words_count.append(len(temp))\n",
    "        #count the number of stopwords in each song\n",
    "        stopwords_c=count_stopwords(temp)\n",
    "        stopwords_count.append(stopwords_c)\n",
    "        #remove all of the stopwords\n",
    "        temp = remove_mystopwords(temp)\n",
    "        #for the further work with text we would like to lowercase all of the words in text   \n",
    "        lyrics_texts.append(temp.lower())\n",
    "        lyrics_texts[i]=lyrics_texts[i].replace('\\n ','\\n')  \n",
    "    return lyrics_texts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics=find_lyrics_txt(df_spotify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add 2 columns to our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[160, 122, 129, 129, 184, 157, 116, 116, 170, 96, 67, 142, 113, 133, 122, 113, 46, 126, 231, 137, 131, 115, 124, 155, 77, 149, 69, 126, 169, 204, 171, 175, 162, 189, 134, 71, 152, 170, 142, 156, 108, 71, 83, 102, 229, 155, 94, 128, 72, 135, 95, 220, 160, 56, 130, 99, 226, 149, 148, 104, 114, 91, 104, 102, 122, 105, 30, 224, 190, 172, 111, 102, 248, 176, 154, 173, 137, 71, 189, 145, 147, 114, 99, 193, 45, 83, 125, 195, 216, 183, 168, 103, 68, 79, 91, 216, 167, 134, 249, 70, 191, 201, 60, 387, 96, 105, 167, 126, 105, 274, 187, 92, 184, 123, 129, 119, 168, 149, 107, 201, 0, 187, 74, 0, 74, 193, 92, 160, 161, 84, 105, 163, 190, 252, 184, 0, 216, 134, 221, 180, 173, 164, 132, 94, 153, 211, 170, 14, 66, 161, 377, 123, 206, 162, 131, 155, 117, 248, 195, 113, 108, 84, 0, 104, 163, 164, 189, 176, 123, 215, 153, 62, 100, 138, 70, 137, 161, 189, 103, 137, 135, 96, 245, 116, 154, 100, 182, 79, 126, 291, 133, 193, 127, 155, 122, 93, 110, 103, 115, 146, 127, 162, 166, 135, 148, 171, 139, 238, 201, 176, 192, 124, 135, 41, 163, 164, 177, 73, 66, 167, 188, 133, 123, 121, 214, 177, 218, 134, 79, 132, 307, 97, 133, 182, 205, 107, 109, 137, 251, 264, 67, 0, 74, 189, 161, 138, 0, 140, 214, 92, 187, 191, 143, 142, 139, 230, 164, 287, 162, 217, 153, 112, 92, 70, 94, 105, 74, 249, 291, 189, 167, 194, 105, 180, 132, 197, 149, 109, 151, 0, 161, 136, 140, 114, 78, 155, 23, 154, 167, 29, 23, 68, 121, 340, 139, 14, 167, 60, 0, 91, 0, 0, 0, 65, 94, 99, 121, 128, 0, 37, 110, 0, 113, 78, 31, 0, 0, 0, 0, 0, 0, 86, 187, 28, 173, 18, 74, 143, 109, 0, 44, 77, 105, 0, 119, 98, 109, 211, 0, 178, 90, 0, 94, 164, 139, 73, 147, 127, 123, 158, 158, 106, 111, 0, 45, 88, 155, 150, 87, 33, 88, 106, 67, 0, 81, 114, 72, 215, 206, 81, 68, 146, 84, 64, 252, 60, 0, 0, 0, 136, 156, 54, 132, 0, 142, 0, 75, 318, 105, 0, 82, 238, 0, 145, 33, 132, 118, 129, 148, 0, 125, 97, 207, 113, 0, 77, 0, 79, 84, 39, 0, 187, 34, 52, 0, 0, 155, 0, 58, 72, 204, 55, 115, 221, 103, 136, 76, 121, 125, 83, 0, 122, 0, 139, 255, 0, 71, 71, 120, 192, 88, 0, 98, 66, 0, 0, 166, 98, 55, 62, 14, 0, 82, 0, 200, 0, 129, 166, 0, 271, 71, 213, 123, 72, 119, 105, 34, 143, 244, 8, 0, 173, 0, 133, 32, 81, 39, 0, 99, 100, 0, 190, 0, 0, 244, 0, 0, 0, 56, 94, 94, 139, 89, 55, 54, 45, 104, 143, 161, 0, 152, 159, 161, 0, 111, 85, 218, 120, 83, 167, 162, 0, 92, 106, 134, 101, 91, 270, 374, 210, 129, 0, 0, 94, 0, 186, 81, 182, 126, 98, 94, 0, 111, 0, 118, 122, 102, 85, 0, 48, 100, 0, 138, 152, 321, 0, 164, 151, 177, 124, 83, 96, 93, 52, 44, 67, 89, 0, 70, 131, 195, 35, 0, 8, 246, 0, 166, 150, 114, 0, 143, 130, 179, 54, 137, 72, 76, 133, 75, 119, 0, 65, 98, 132, 117, 117, 86]\n"
     ]
    }
   ],
   "source": [
    "append_col(df_spotify,all_words_count,'words_count')\n",
    "append_col(df_spotify,stopwords_count,'stopwords_count')\n",
    "print(stopwords_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2057, 1479, 1773, 2704, 2494, 3278, 1592, 1568, 2538, 1770, 1572, 2059, 1355, 1664, 1950, 1994, 1207, 2012, 3150, 1987, 1686, 1622, 1832, 2112, 1216, 1842, 1011, 1362, 2459, 4137, 1933, 2061, 2178, 2712, 1762, 1980, 2572, 2569, 2143, 1667, 1965, 1461, 1077, 1356, 2634, 2145, 1490, 2669, 1300, 2069, 2183, 2788, 1880, 2100, 1811, 1449, 3696, 2197, 1991, 1734, 1607, 1370, 1551, 1416, 2015, 1708, 807, 2949, 1771, 2208, 1385, 1548, 2692, 2367, 2098, 2339, 2534, 945, 2352, 2000, 1795, 1597, 1208, 3024, 528, 1834, 1913, 2772, 2439, 3158, 1996, 1572, 1130, 1415, 1866, 3500, 1876, 2171, 2958, 3072, 2318, 2737, 1622, 5653, 1770, 1324, 1970, 2344, 1628, 4046, 2240, 1865, 2325, 2171, 1372, 2020, 2858, 2197, 1519, 2360, 0, 3034, 1229, 0, 1497, 3133, 1542, 2266, 2441, 1289, 1602, 2371, 3265, 3228, 2097, 0, 2300, 1675, 2669, 1944, 2339, 3084, 1502, 1490, 2279, 3223, 2538, 1187, 1494, 2859, 3819, 1587, 2841, 2363, 1686, 1932, 2315, 2833, 3436, 1650, 1742, 1166, 0, 1372, 2669, 2639, 2712, 2425, 1690, 2601, 2240, 1028, 1499, 1803, 1418, 2171, 2471, 2703, 1572, 1959, 1505, 2296, 4121, 1766, 1845, 1232, 1941, 1106, 1362, 3511, 1733, 2429, 1972, 1648, 1769, 1069, 1760, 1657, 1409, 1572, 1972, 2363, 2667, 3709, 1746, 2326, 2762, 3050, 2718, 2480, 2423, 2032, 2164, 1227, 2052, 2061, 2252, 993, 1494, 2418, 2374, 2016, 1775, 2024, 3179, 2770, 3455, 2346, 1314, 1502, 4726, 1259, 1987, 2191, 2575, 1511, 1561, 1996, 3565, 4015, 1707, 0, 1378, 2295, 3128, 1803, 0, 2984, 3301, 1166, 3034, 2840, 1904, 1840, 1671, 2881, 1725, 4178, 1978, 3454, 2279, 1658, 1551, 1418, 1369, 1594, 1129, 2958, 3511, 3377, 2295, 2918, 1429, 2223, 1435, 3207, 1758, 5184, 1730, 0, 2441, 2795, 1859, 1438, 1490, 1648, 1160, 1929, 2361, 2411, 901, 1087, 1831, 4910, 1929, 1187, 1970, 826, 0, 1201, 0, 0, 0, 574, 1434, 1504, 1281, 1486, 0, 747, 1765, 0, 2105, 1590, 524, 0, 0, 0, 0, 0, 0, 1462, 3341, 450, 3300, 300, 1621, 1482, 1746, 0, 797, 1204, 1775, 0, 1500, 1291, 1787, 2817, 0, 3177, 1059, 0, 1309, 2099, 1773, 908, 1794, 2111, 1588, 2466, 2227, 1688, 1469, 0, 615, 1570, 2910, 1538, 1157, 1846, 1204, 1831, 965, 0, 849, 2027, 999, 2829, 2629, 1146, 1417, 2186, 1866, 1458, 2843, 1072, 0, 0, 0, 1597, 2199, 892, 1673, 0, 2119, 0, 1196, 4360, 2104, 0, 1123, 2850, 0, 2218, 526, 1964, 1959, 2022, 2036, 0, 2124, 1464, 3079, 2085, 0, 1173, 0, 1402, 1004, 853, 0, 2240, 815, 868, 0, 0, 1496, 0, 939, 1148, 2033, 996, 1277, 2252, 1502, 1717, 934, 2295, 1597, 1251, 0, 1495, 0, 1792, 3560, 0, 882, 1118, 1508, 2117, 1230, 0, 1261, 943, 0, 0, 2030, 1317, 752, 975, 1187, 0, 1081, 0, 2777, 0, 1372, 2303, 0, 3605, 1010, 3023, 1414, 1086, 1928, 1902, 1163, 2303, 2823, 1088, 0, 2519, 0, 1634, 2136, 1320, 872, 0, 1702, 1483, 0, 2532, 0, 0, 3460, 0, 0, 0, 989, 1571, 1673, 1797, 1458, 864, 968, 659, 1955, 1904, 1870, 0, 2042, 1742, 2389, 0, 1700, 912, 2742, 1528, 1415, 1816, 2268, 0, 1475, 1966, 1200, 2290, 1704, 3730, 5651, 3014, 2144, 0, 0, 1561, 0, 2471, 1599, 1946, 1922, 1891, 1507, 0, 1412, 0, 1549, 1824, 2321, 1592, 0, 849, 1302, 0, 2423, 1880, 4758, 0, 2291, 2223, 2817, 1465, 1359, 1458, 1740, 871, 2214, 1049, 1981, 0, 1255, 1846, 2950, 421, 0, 1714, 2574, 0, 2341, 2084, 1530, 0, 1803, 2619, 2990, 1021, 1886, 1538, 1326, 1887, 962, 2028, 0, 1231, 1314, 1889, 1986, 1802, 1125]\n"
     ]
    }
   ],
   "source": [
    "print(all_words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_without_empty_strings = []\n",
    "for string in lyrics:\n",
    "    if (string != \"\"):\n",
    "        lyrics_without_empty_strings.append(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we want to check the amount of songs with zero words (the algorithm of finding the url for metrolyrics didn't work for them) so we can delete them in the dataframe in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120, 123, 135, 162, 241, 246, 279, 298, 300, 301, 302, 308, 311, 315, 316, 317, 318, 319, 320, 329, 333, 338, 341, 353, 363, 376, 377, 378, 383, 385, 389, 392, 399, 404, 406, 410, 414, 415, 417, 430, 432, 435, 441, 444, 445, 451, 453, 455, 458, 470, 472, 477, 480, 482, 483, 485, 486, 487, 499, 503, 511, 521, 522, 524, 531, 533, 538, 541, 545, 557, 562, 565, 569, 580]\n"
     ]
    }
   ],
   "source": [
    "res_list = [] \n",
    "count=0\n",
    "for i in range(0, len(all_words_count)) : \n",
    "    if all_words_count[i] == 0 : \n",
    "        res_list.append(i)    \n",
    "print(res_list)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning: In the next cell we drop every row which value of amout of words in song is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range (0,len(df_spotify)):\n",
    "    if (df_spotify['words_count'][i]) == 0:\n",
    "        df_spotify['words_count'][i]=pd.np.NaN\n",
    "        \n",
    "df_spotify=df_spotify.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the amount of rows in a new dataframe without nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_spotify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a text as positive, negative or neutral using textblob package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the polarity value for each song and defining the range for each pnn value: \n",
    "\n",
    "* pnn between -0.3 and 0.3 is neutral and its value will marked as 0\n",
    "* pnn above 0.3 is positive and its value will marked as 1\n",
    "* pnn below -0.3 is negative and its value will marked as -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "# The sentiment function of textblob returns two properties, polarity, and subjectivity.\n",
    "# Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement\n",
    "\n",
    "def pos_neg_neutral(lyrics_texts):\n",
    "    blobs=[]\n",
    "    for i in range (0,len(lyrics_texts)):\n",
    "        blob=TextBlob(lyrics_texts[i])\n",
    "        pnn=blob.sentiment.polarity\n",
    "        definition=0\n",
    "        if -0.3 < pnn < 0.3:\n",
    "            definition=0\n",
    "        if pnn  <= -0.3:\n",
    "            definition=-1\n",
    "        if pnn >= 0.3:\n",
    "            definition=1\n",
    "        blobs.append(definition)    \n",
    "    return blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs_polarity=pos_neg_neutral(lyrics_without_empty_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blobs_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(blobs_polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we will check hom many negative,neutral,positive songs do we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': 50, 'neg': 7, 'neutral': 456}\n"
     ]
    }
   ],
   "source": [
    "blobs_res={\"pos\": 0, \"neg\": 0, \"neutral\": 0}\n",
    "for i in range (0,len(blobs_polarity)):\n",
    "    if blobs_polarity[i]==1:\n",
    "        blobs_res[\"pos\"]+=1\n",
    "    if blobs_polarity[i]==-1:\n",
    "        blobs_res[\"neg\"]+=1\n",
    "    if blobs_polarity[i]==0:\n",
    "        blobs_res[\"neutral\"]+=1\n",
    "print(blobs_res)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_col(df_spotify,blobs_polarity,'blobs_polarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we will tokenize (convert the whole lyrics text to words) the lyrics of each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lyrics=[]\n",
    "\n",
    "for i in range (0,len(df_spotify)):\n",
    "    tokens=word_tokenize(lyrics_without_empty_strings[i])\n",
    "    tokenized_lyrics.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'times', 'rain', 'parade', 'and', 'clubs', 'get', 'using', 'name', 'you', 'think', 'broke', 'heart', 'oh', 'girl', 'goodness', 'sake', 'you', 'think', 'im', 'crying', 'well', 'i', 'aint', 'and', 'i', 'didnt', 'wan', 'na', 'write', 'song', 'cause', 'i', 'didnt', 'want', 'anyone', 'thinking', 'i', 'still', 'care', 'i', 'dont', 'still', 'hit', 'phone', 'up', 'and', 'baby', 'i', 'movin', 'on', 'and', 'i', 'think', 'somethin', 'i', 'dont', 'wan', 'na', 'hold', 'back', 'maybe', 'know', 'that', 'my', 'mama', 'dont', 'like', 'likes', 'everyone', 'and', 'i', 'never', 'like', 'admit', 'i', 'wrong', 'and', 'ive', 'caught', 'job', 'didnt', 'see', 'whats', 'going', 'on', 'but', 'i', 'know', 'im', 'better', 'sleeping', 'own', 'cause', 'like', 'way', 'look', 'much', 'oh', 'baby', 'go', 'love', 'yourself', 'and', 'think', 'im', 'still', 'holdin', 'somethin', 'you', 'go', 'love', 'yourself', 'and', 'told', 'hated', 'friends', 'the', 'problem', 'them', 'and', 'every', 'time', 'told', 'opinion', 'wrong', 'and', 'tried', 'make', 'forget', 'i', 'came', 'from', 'and', 'i', 'didnt', 'wan', 'na', 'write', 'song', 'cause', 'i', 'didnt', 'want', 'anyone', 'thinking', 'i', 'still', 'care', 'i', 'dont', 'still', 'hit', 'phone', 'up', 'and', 'baby', 'i', 'movin', 'on', 'and', 'i', 'think', 'somethin', 'i', 'dont', 'wan', 'na', 'hold', 'back', 'maybe', 'know', 'that', 'my', 'mama', 'dont', 'like', 'likes', 'everyone', 'and', 'i', 'never', 'like', 'admit', 'i', 'wrong', 'and', 'ive', 'caught', 'job', 'didnt', 'see', 'whats', 'going', 'on', 'but', 'i', 'know', 'im', 'better', 'sleeping', 'own', 'cause', 'like', 'way', 'look', 'much', 'oh', 'baby', 'go', 'love', 'yourself', 'and', 'think', 'im', 'still', 'holdin', 'somethin', 'you', 'go', 'love', 'yourself', 'for', 'times', 'made', 'feel', 'small', 'i', 'fell', 'love', 'i', 'feel', 'nothin', 'all', 'had', 'never', 'felt', 'low', 'i', 'vulnerable', 'was', 'i', 'fool', 'let', 'break', 'walls', 'cause', 'like', 'way', 'look', 'much', 'oh', 'baby', 'go', 'love', 'yourself', 'and', 'think', 'im', 'still', 'holdin', 'somethin', 'you', 'go', 'love', 'yourself', 'cause', 'like', 'way', 'look', 'much', 'oh', 'baby', 'go', 'love', 'yourself', 'and', 'think', 'im', 'still', 'holdin', 'somethin', 'you', 'go', 'love', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "print(str(tokenized_lyrics[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next 3 cells perfom 3 things:\n",
    "\n",
    "* create a list with positive words by scraping a web page that includes the necessary information for this and count the total amount of these words in each song\n",
    "* create a list with negative words by scraping a web page that includes the necessary information for this and count the total amount of these words in each song\n",
    "* create a list with curse words by scraping a web page that includes the necessary information for this and count the total amount of these words in each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350, 600, 525, 450, 525, 200, 200, 400, 850, 425, 375, 300, 275, 825, 675, 175, 150, 450, 800, 200, 400, 875, 150, 900, 225, 950, 300, 325, 575, 550, 275, 175, 675, 350, 450, 200, 300, 350, 375, 425, 200, 425, 775, 125, 325, 275, 175, 325, 300, 575, 400, 1050, 275, 625, 200, 175, 1875, 450, 1025, 325, 275, 75, 250, 450, 275, 350, 100, 400, 225, 725, 325, 250, 1050, 525, 475, 275, 1000, 175, 525, 375, 175, 950, 150, 2375, 175, 1250, 625, 375, 275, 425, 125, 675, 100, 500, 275, 525, 200, 800, 125, 50, 675, 475, 550, 700, 425, 275, 500, 825, 375, 2450, 325, 200, 650, 600, 200, 275, 475, 450, 25, 425, 725, 375, 200, 400, 325, 650, 575, 350, 125, 575, 725, 925, 425, 225, 275, 250, 250, 275, 600, 450, 175, 575, 700, 850, 0, 400, 225, 1275, 450, 500, 250, 400, 1325, 625, 450, 475, 500, 625, 175, 300, 300, 275, 350, 375, 375, 650, 675, 225, 350, 1250, 175, 0, 525, 225, 675, 475, 400, 275, 875, 450, 175, 125, 175, 300, 325, 250, 325, 425, 125, 400, 350, 0, 175, 175, 525, 400, 125, 250, 250, 275, 125, 425, 475, 600, 575, 675, 1200, 700, 575, 625, 475, 300, 625, 200, 400, 225, 550, 275, 200, 775, 250, 700, 1575, 525, 150, 450, 550, 175, 150, 275, 275, 100, 300, 575, 975, 200, 50, 575, 250, 225, 1250, 750, 375, 500, 725, 525, 100, 375, 50, 500, 125, 475, 850, 675, 575, 400, 600, 175, 450, 500, 275, 125, 250, 475, 125, 700, 300, 600, 100, 750, 450, 150, 1175, 575, 275, 350, 300, 1275, 400, 50, 400, 375, 25, 0, 100, 300, 750, 725, 0, 500, 50, 325, 100, 50, 100, 100, 425, 75, 1050, 1325, 425, 250, 675, 625, 250, 1500, 100, 225, 450, 425, 50, 475, 100, 200, 500, 975, 475, 925, 275, 250, 525, 625, 425, 750, 175, 50, 375, 750, 175, 450, 525, 400, 500, 400, 250, 50, 225, 500, 100, 125, 500, 300, 525, 525, 150, 225, 750, 200, 350, 300, 50, 325, 700, 200, 400, 900, 125, 725, 525, 275, 1025, 650, 50, 350, 225, 200, 325, 475, 300, 1250, 525, 350, 50, 25, 525, 325, 75, 225, 875, 150, 375, 250, 150, 450, 325, 25, 975, 375, 225, 475, 225, 475, 175, 525, 200, 225, 125, 325, 475, 250, 150, 550, 450, 200, 200, 0, 250, 525, 200, 125, 625, 225, 750, 250, 150, 875, 50, 0, 625, 325, 100, 425, 150, 0, 325, 150, 300, 125, 625, 375, 150, 750, 425, 675, 350, 350, 175, 100, 425, 300, 275, 750, 225, 200, 200, 50, 275, 1275, 800, 275, 600, 150, 325, 0, 175, 200, 625, 975, 350, 550, 700, 100, 300, 450, 400, 400, 1325, 725, 575, 650, 250, 200, 125, 325, 275, 400, 950, 200, 50, 375, 275, 325, 1150, 250, 400, 0, 300, 225, 250, 625, 1100, 25, 75, 275, 350, 725, 600, 350, 775, 325, 275, 425, 475, 300, 175, 150, 1150, 250, 225, 300, 450, 525, 350]\n"
     ]
    }
   ],
   "source": [
    "count_pos=[]\n",
    "\n",
    "keywords_pos=[]\n",
    "\n",
    "url='https://www.enchantedlearning.com/wordlist/positivewords.shtml'\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "soup = bs(data,'html.parser')\n",
    "words = soup.findAll(\"div\",{\"class\":\"wordlist-section\"})\n",
    "\n",
    "for div in words:\n",
    "    divs=soup.findAll(\"div\",{\"class\":\"wordlist-item\"})\n",
    "    for div in divs:\n",
    "        keywords_pos.append(div.text)\n",
    "\n",
    "for i in range (0,len(df_spotify)):\n",
    "    temp=[]\n",
    "    for word in keywords_pos:\n",
    "        #here we create a temp list to store all the words from keywords_neg which appear in lyrics \n",
    "        temp+=re.findall(word, str(lyrics_without_empty_strings[i]))\n",
    "    #now we'd want to know how many times these words appeared in text\n",
    "    count_pos.append(len(temp))\n",
    "print(count_pos)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[648, 1104, 384, 816, 336, 600, 312, 288, 480, 840, 552, 1032, 504, 456, 1008, 240, 72, 624, 1032, 336, 264, 672, 672, 600, 600, 360, 456, 360, 288, 552, 960, 168, 336, 312, 456, 288, 456, 1272, 456, 1320, 408, 672, 576, 336, 960, 600, 624, 672, 240, 456, 216, 1656, 360, 192, 216, 288, 312, 360, 672, 552, 408, 192, 168, 408, 360, 1104, 576, 1056, 240, 312, 504, 168, 960, 504, 456, 480, 1056, 96, 600, 432, 456, 120, 168, 2976, 168, 1008, 840, 648, 216, 336, 168, 528, 72, 168, 2064, 1080, 432, 432, 192, 360, 288, 408, 120, 1968, 840, 240, 456, 792, 552, 840, 672, 408, 1152, 288, 144, 432, 1032, 360, 408, 336, 672, 264, 312, 816, 360, 600, 624, 480, 288, 288, 624, 576, 144, 912, 312, 768, 96, 480, 1152, 720, 624, 1488, 192, 480, 192, 120, 576, 504, 216, 384, 312, 264, 1296, 888, 336, 576, 456, 552, 432, 312, 168, 1728, 312, 216, 240, 384, 864, 288, 144, 1008, 144, 360, 648, 168, 528, 288, 288, 480, 624, 576, 168, 240, 96, 168, 360, 264, 384, 672, 96, 528, 984, 0, 1608, 504, 336, 624, 96, 312, 912, 552, 264, 312, 240, 816, 1104, 1032, 528, 696, 480, 768, 864, 480, 528, 96, 120, 264, 384, 384, 336, 1200, 240, 840, 240, 432, 312, 720, 1080, 120, 264, 480, 264, 408, 576, 744, 552, 168, 144, 336, 864, 432, 1008, 264, 528, 168, 672, 264, 432, 24, 312, 432, 120, 816, 312, 312, 1488, 192, 144, 144, 72, 624, 24, 192, 264, 1056, 480, 456, 672, 528, 264, 384, 720, 936, 168, 624, 744, 504, 408, 336, 528, 312, 288, 720, 192, 144, 96, 216, 672, 432, 192, 456, 432, 192, 0, 264, 936, 192, 264, 96, 672, 360, 792, 288, 1080, 648, 192, 648, 48, 264, 336, 528, 168, 504, 960, 120, 48, 288, 840, 528, 456, 552, 1032, 144, 0, 672, 648, 360, 552, 840, 192, 384, 72, 840, 288, 240, 288, 72, 312, 1056, 216, 0, 600, 312, 1272, 360, 312, 408, 1080, 360, 480, 528, 72, 744, 936, 216, 456, 888, 288, 1008, 1224, 576, 960, 648, 48, 552, 192, 408, 720, 456, 840, 384, 480, 744, 72, 144, 432, 672, 0, 360, 744, 48, 624, 576, 96, 168, 336, 48, 696, 120, 408, 264, 504, 624, 816, 312, 432, 576, 408, 312, 360, 648, 312, 408, 72, 168, 624, 192, 480, 576, 144, 480, 384, 312, 2064, 240, 168, 456, 192, 240, 480, 384, 0, 504, 360, 216, 600, 168, 288, 528, 672, 384, 144, 672, 216, 576, 576, 216, 408, 120, 696, 408, 432, 504, 336, 624, 456, 168, 360, 816, 744, 288, 288, 480, 120, 0, 336, 480, 576, 1320, 240, 288, 216, 672, 72, 72, 696, 1920, 768, 624, 552, 504, 1104, 432, 168, 240, 336, 528, 480, 432, 264, 240, 552, 192, 264, 768, 456, 264, 120, 840, 384, 672, 768, 72, 288, 96, 528, 984, 480, 192, 1296, 432, 264, 312, 528, 840, 168, 96, 672, 144, 480, 72, 120, 360, 48]\n"
     ]
    }
   ],
   "source": [
    "count_neg=[]\n",
    "\n",
    "keywords_neg=[]\n",
    "\n",
    "url='https://www.enchantedlearning.com/wordlist/negativewords.shtml'\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "soup = bs(data,'html.parser')\n",
    "words = soup.findAll(\"div\",{\"class\":\"wordlist-section\"})\n",
    "\n",
    "for div in words:\n",
    "    divs=soup.findAll(\"div\",{\"class\":\"wordlist-item\"})\n",
    "    for div in divs:\n",
    "        keywords_neg.append(div.text)\n",
    "        \n",
    "for i in range (0,len(df_spotify)):\n",
    "    temp=[]\n",
    "    for word in keywords_neg:\n",
    "        #here we create a temp list to store all the words from keywords_neg which appear in lyrics \n",
    "        temp+=re.findall(word, str(lyrics_without_empty_strings[i]))\n",
    "    #now we'd want to know how many times these words appeared in text\n",
    "    count_neg.append(len(temp))\n",
    "print(count_neg)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 24, 9, 0, 0, 0, 0, 0, 14, 2, 1, 0, 0, 8, 8, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 28, 0, 17, 0, 25, 6, 0, 4, 10, 0, 3, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 5, 12, 1, 0, 11, 13, 6, 16, 6, 3, 0, 0, 11, 6, 0, 1, 5, 1, 4, 0, 7, 11, 0, 2, 0, 1, 0, 10, 0, 0, 0, 0, 1, 0, 10, 10, 16, 6, 2, 3, 0, 0, 11, 41, 1, 0, 0, 1, 1, 51, 0, 50, 0, 1, 1, 8, 0, 5, 2, 19, 7, 4, 1, 3, 6, 13, 0, 0, 22, 8, 0, 8, 1, 0, 1, 0, 3, 0, 23, 0, 1, 0, 1, 1, 1, 2, 0, 0, 2, 7, 25, 0, 0, 2, 23, 70, 0, 10, 13, 0, 0, 21, 1, 13, 0, 16, 20, 2, 4, 1, 25, 4, 1, 3, 3, 6, 0, 0, 0, 20, 9, 23, 3, 7, 0, 4, 21, 0, 0, 0, 0, 0, 0, 18, 2, 27, 1, 0, 16, 25, 0, 0, 3, 0, 1, 13, 7, 8, 0, 16, 1, 10, 2, 1, 3, 0, 1, 1, 0, 3, 0, 2, 2, 28, 2, 6, 0, 1, 23, 1, 44, 15, 0, 0, 14, 3, 1, 0, 20, 3, 4, 0, 17, 7, 27, 0, 4, 3, 0, 1, 24, 0, 22, 2, 46, 21, 0, 16, 0, 44, 0, 21, 7, 0, 0, 0, 1, 1, 0, 0, 18, 4, 15, 4, 0, 0, 0, 20, 0, 1, 1, 1, 0, 6, 0, 18, 0, 0, 5, 0, 0, 5, 1, 0, 30, 9, 0, 1, 0, 0, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 1, 11, 0, 8, 2, 1, 0, 6, 0, 0, 0, 17, 0, 2, 10, 36, 0, 0, 5, 22, 0, 24, 0, 0, 17, 6, 8, 0, 0, 0, 19, 0, 1, 0, 0, 1, 0, 0, 0, 0, 41, 1, 1, 0, 16, 0, 0, 33, 0, 0, 5, 0, 2, 1, 1, 25, 8, 1, 3, 10, 0, 2, 1, 1, 0, 7, 0, 12, 0, 7, 22, 4, 0, 2, 0, 0, 0, 0, 0, 8, 2, 0, 0, 0, 0, 3, 0, 0, 0, 0, 21, 7, 0, 0, 0, 15, 0, 0, 5, 0, 1, 0, 0, 0, 0, 14, 1, 13, 10, 4, 2, 7, 0, 1, 0, 1, 2, 0, 0, 2, 0, 0, 0, 5, 0, 0, 14, 36, 0, 0, 0, 2, 0, 5, 0, 2, 2, 0, 0, 2, 0, 10, 4, 0, 16, 0, 0, 4, 34, 5, 1, 0, 0, 2, 31, 33, 23, 21, 3, 13, 52, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 9, 23, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 12, 6, 0, 1, 12, 2, 1, 0, 2, 0, 13, 1, 0, 0, 0, 0, 1, 0, 3, 2, 0, 24, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "count_swear=[]\n",
    "\n",
    "keywords_swear=[]\n",
    "\n",
    "url='https://en.wiktionary.org/wiki/Category:English_swear_words'\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "soup = bs(data,'html.parser')\n",
    "div_class = soup.findAll(\"div\",{\"class\":\"mw-category-group\"})\n",
    "\n",
    "for div in div_class:\n",
    "    uls=div.find_all('ul')\n",
    "    for ul in uls:\n",
    "        lis=ul.find_all('li')\n",
    "        for ls in lis:\n",
    "            links=ls.find_all('a')\n",
    "            for link in links:\n",
    "                keywords_swear.append(link.text)\n",
    "                \n",
    "keywords_swear.pop(0)         \n",
    "\n",
    "for k in range(0,len(keywords_swear)):\n",
    "    keywords_swear[k] =keywords_swear[k].lower() \n",
    "\n",
    "# print(keywords_swear)\n",
    "\n",
    "for i in range (0,len(df_spotify)):\n",
    "    temp=[]\n",
    "    for word in keywords_swear:\n",
    "        #here we create a temp list to store all the words from keywords_neg which appear in lyrics \n",
    "        temp+=re.findall(word, str(lyrics_without_empty_strings[i]))\n",
    "    #now we'd want to know how many times these words appeared in text\n",
    "    count_swear.append(len(temp))\n",
    "print(count_swear)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_col(df_spotify,count_neg,'words_neg_count')\n",
    "append_col(df_spotify,count_pos,'words_pos_count')\n",
    "append_col(df_spotify,count_swear,'words_curse_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we'd like to find the total count of repeated words and the most common words in each song using Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "reg = re.compile('\\S{4,}')\n",
    "\n",
    "repeated_words=[]\n",
    "most_common_words=[]\n",
    "\n",
    "for i in range (0,len(df_spotify)):\n",
    "    c = Counter(ma.group() for ma in reg.finditer(lyrics_without_empty_strings[i]))\n",
    "    repeated_words.append(sum(c.values()))\n",
    "    most_common_words.append([k for k,v in c.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(most_common_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.08, 5.34, 4.98, 5.36, 5.29, 5.42, 5.51, 5.27, 5.42, 5.03, 4.86, 5.54, 5.19, 5.09, 5.34, 5.28, 5.5, 5.64, 5.4, 5.41, 5.37, 5.41, 5.19, 5.41, 5.41, 5.19, 5.14, 5.31, 5.53, 5.53, 5.49, 4.97, 5.36, 5.36, 5.4, 5.82, 5.66, 5.27, 5.71, 5.11, 5.12, 5.49, 4.83, 5.52, 5.38, 5.25, 5.5, 5.23, 5.36, 4.95, 5.15, 5.3, 5.49, 5.68, 5.07, 5.1, 5.65, 5.15, 5.2, 5.38, 5.63, 4.96, 5.35, 5.41, 5.34, 5.33, 4.93, 5.55, 5.22, 4.92, 4.9, 5.13, 4.83, 5.56, 5.54, 5.32, 5.01, 5.41, 5.25, 5.61, 5.19, 5.34, 5.05, 4.97, 5.4, 4.94, 5.01, 5.31, 5.27, 6.06, 5.52, 5.5, 5.2, 5.24, 5.87, 5.32, 5.38, 5.44, 5.18, 5.98, 5.34, 5.45, 5.8, 5.25, 5.03, 4.96, 5.22, 5.96, 5.01, 5.34, 5.35, 5.95, 4.87, 5.17, 5.15, 5.21, 5.31, 5.15, 5.54, 5.27, 4.98, 5.07, 5.6, 5.34, 4.88, 5.85, 5.26, 5.12, 5.26, 4.78, 5.39, 4.97, 4.93, 5.11, 5.54, 5.31, 5.71, 5.32, 5.67, 4.97, 5.5, 5.28, 5.26, 5.42, 5.74, 5.67, 5.29, 5.22, 5.18, 5.36, 5.39, 5.37, 4.91, 5.14, 5.3, 6.27, 5.26, 5.11, 5.63, 5.43, 5.31, 5.37, 5.36, 5.63, 4.68, 5.39, 5.58, 5.67, 5.2, 5.05, 5.56, 5.06, 5.28, 5.8, 5.5, 5.42, 5.64, 5.46, 5.45, 5.19, 4.94, 5.22, 5.13, 5.22, 5.31, 5.64, 4.86, 5.09, 5.18, 4.85, 5.46, 5.02, 4.98, 5.39, 5.47, 4.85, 5.18, 5.39, 5.18, 5.57, 5.65, 5.44, 5.21, 5.43, 5.44, 5.36, 5.26, 5.17, 5.32, 5.5, 5.53, 5.28, 4.98, 5.28, 5.67, 5.52, 5.07, 5.5, 5.26, 5.35, 5.39, 5.39, 5.18, 4.95, 5.31, 4.97, 5.3, 4.98, 5.42, 6.28, 5.59, 5.3, 5.53, 5.53, 5.25, 5.28, 5.36, 5.67, 6.0, 5.19, 5.05, 5.51, 5.55, 5.12, 4.98, 5.62, 5.04, 4.94, 5.15, 5.46, 5.22, 5.34, 5.15, 5.28, 5.28, 5.21, 5.1, 5.56, 5.53, 5.23, 5.25, 5.18, 5.64, 5.67, 5.65, 5.49, 5.56, 5.56, 4.91, 5.4, 5.17, 5.72, 5.05, 5.26, 6.29, 5.12, 5.34, 5.42, 4.85, 5.39, 5.27, 5.14, 5.72, 5.0, 4.82, 5.54, 5.46, 5.11, 5.74, 5.22, 4.87, 4.97, 4.35, 5.45, 4.86, 5.12, 5.26, 5.02, 5.28, 5.19, 5.35, 4.88, 5.15, 5.21, 4.88, 5.62, 5.13, 5.54, 4.92, 5.0, 5.24, 5.27, 5.51, 5.19, 5.42, 5.64, 5.11, 5.64, 5.26, 5.15, 5.16, 4.97, 5.5, 5.27, 5.49, 5.35, 5.24, 5.28, 5.71, 5.22, 5.13, 5.26, 5.56, 5.04, 5.28, 5.46, 5.71, 5.39, 5.13, 6.0, 5.26, 5.56, 5.38, 5.52, 5.3, 5.45, 5.14, 5.13, 5.64, 4.88, 5.14, 5.43, 5.42, 4.6, 5.25, 5.43, 5.59, 5.22, 5.26, 5.05, 5.19, 5.09, 4.76, 5.63, 5.22, 5.53, 5.39, 5.4, 4.94, 5.42, 5.49, 5.86, 5.32, 4.56, 5.51, 5.35, 5.0, 5.15, 5.14, 5.59, 5.37, 5.09, 4.98, 5.25, 5.39, 5.71, 5.27, 4.81, 5.37, 5.4, 5.43, 5.55, 4.97, 5.56, 4.79, 5.23, 5.6, 5.43, 5.07, 4.8, 5.37, 4.91, 4.98, 4.79, 5.19, 5.74, 5.33, 5.23, 5.15, 5.68, 5.38, 5.18, 5.44, 5.29, 5.31, 5.45, 5.27, 5.45, 5.27, 5.22, 4.82, 5.38, 5.21, 5.71, 5.84, 4.66, 4.9, 5.65, 5.34, 5.31, 5.88, 5.41, 5.2, 5.08, 5.13, 5.04, 5.76, 5.23, 5.11, 5.94, 5.19, 5.06, 5.44, 5.14, 5.34, 5.27, 5.6, 4.92, 5.48, 4.98, 5.15, 5.34, 5.61, 4.64, 5.37, 5.62, 5.5, 5.6, 5.23, 5.54, 5.6, 5.28, 5.48, 5.38, 5.0, 5.28, 5.54, 5.21, 5.25, 5.3, 5.27, 5.4, 5.29, 5.32, 5.25, 5.11, 5.49, 5.1, 5.82, 5.53, 5.52, 5.3, 5.31, 5.49, 5.66, 5.97, 5.78, 5.31, 5.33, 5.19, 5.37, 5.27, 5.66, 5.07, 5.65, 5.29, 5.14, 5.19, 5.62, 5.37, 5.0, 5.62, 6.12, 5.61, 5.06, 5.09, 5.38, 5.46, 5.18, 5.49, 5.5, 5.61, 4.91]\n"
     ]
    }
   ],
   "source": [
    "avg_most_common_words_size=[]\n",
    "\n",
    "for i in range(0,len(most_common_words)):\n",
    "    total=0\n",
    "    for word in most_common_words[i]:\n",
    "        total += len(word)\n",
    "    ave_size = round(float(total) / float(len(most_common_words[i])) , 2) \n",
    "    avg_most_common_words_size.append(ave_size)\n",
    "        \n",
    "print(avg_most_common_words_size)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_col(df_spotify,avg_most_common_words_size,'avg_most_common_words_size')\n",
    "append_col(df_spotify,repeated_words,'words_repeat_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify.to_csv('./data/spotify_after.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From this point we saw that we schould add an additional data to our dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify_add_pct = pd.read_csv('./data/spotify_after.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Artist', 'id', 'artist_genres', 'is_single', 'total_tracks',\n",
       "       'release_date', 'day_of_week', 'release_season', 'Year', 'is_top100',\n",
       "       'acousticness', 'danceability', 'duration_ms', 'energy',\n",
       "       'instrumentalness', 'key', 'liveness', 'loudness', 'mode',\n",
       "       'speechiness', 'tempo', 'time_signature', 'valence', 'words_count',\n",
       "       'stopwords_count', 'blobs_polarity', 'words_neg_count',\n",
       "       'words_pos_count', 'words_curse_count', 'avg_most_common_words_size',\n",
       "       'words_repeat_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spotify_add_pct.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we add new columns to the dataframe to see the percentage of:\n",
    "\n",
    "* amount of negative words in the song\n",
    "* amount of positive words in the song\n",
    "* amount of curse words in the song\n",
    "* amount of repeated words in the song\n",
    "* amount of stopwords in the song\n",
    "\n",
    "## to the total number of words in a song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_spotify_add_pct['words_negative_pct'] = round((df_spotify_add_pct['words_neg_count'] / df_spotify_add_pct['words_count'])*100,2)\n",
    "df_spotify_add_pct['words_positive_pct'] = round((df_spotify_add_pct['words_pos_count'] / df_spotify_add_pct['words_count'])*100,2)\n",
    "df_spotify_add_pct['words_curse_pct'] = round((df_spotify_add_pct['words_curse_count'] / df_spotify_add_pct['words_count'])*100,2)\n",
    "df_spotify_add_pct['words_repeat_pct'] = round((df_spotify_add_pct['words_repeat_count'] / df_spotify_add_pct['words_count'])*100,2)\n",
    "df_spotify_add_pct['stopwords_pct'] = round((df_spotify_add_pct['stopwords_count'] / df_spotify_add_pct['words_count'])*100,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Artist</th>\n",
       "      <th>id</th>\n",
       "      <th>artist_genres</th>\n",
       "      <th>is_single</th>\n",
       "      <th>total_tracks</th>\n",
       "      <th>release_date</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>release_season</th>\n",
       "      <th>Year</th>\n",
       "      <th>...</th>\n",
       "      <th>words_neg_count</th>\n",
       "      <th>words_pos_count</th>\n",
       "      <th>words_curse_count</th>\n",
       "      <th>avg_most_common_words_size</th>\n",
       "      <th>words_repeat_count</th>\n",
       "      <th>words_negative_pct</th>\n",
       "      <th>words_positive_pct</th>\n",
       "      <th>words_curse_pct</th>\n",
       "      <th>words_repeat_pct</th>\n",
       "      <th>stopwords_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Love Yourself</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>3hB5DgAiMAQ4DzYbsMq1IT</td>\n",
       "      <td>['canadian pop', 'pop', 'post-teen pop']</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2015-11-13</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "      <td>...</td>\n",
       "      <td>648</td>\n",
       "      <td>350</td>\n",
       "      <td>0</td>\n",
       "      <td>5.08</td>\n",
       "      <td>179</td>\n",
       "      <td>31.50</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.70</td>\n",
       "      <td>7.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Sorry</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>69bp2EbF7Q2rqc5N3ylezZ</td>\n",
       "      <td>['canadian pop', 'pop', 'post-teen pop']</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2015-11-13</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "      <td>...</td>\n",
       "      <td>1104</td>\n",
       "      <td>600</td>\n",
       "      <td>0</td>\n",
       "      <td>5.34</td>\n",
       "      <td>117</td>\n",
       "      <td>74.65</td>\n",
       "      <td>40.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.91</td>\n",
       "      <td>8.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>One Dance</td>\n",
       "      <td>Drake</td>\n",
       "      <td>1xznGGDReH1oQq0xzbwXa3</td>\n",
       "      <td>['canadian hip hop', 'canadian pop', 'hip hop'...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-05-06</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>...</td>\n",
       "      <td>384</td>\n",
       "      <td>525</td>\n",
       "      <td>0</td>\n",
       "      <td>4.98</td>\n",
       "      <td>184</td>\n",
       "      <td>21.66</td>\n",
       "      <td>29.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.38</td>\n",
       "      <td>7.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Work</td>\n",
       "      <td>Rihanna</td>\n",
       "      <td>14WWzenpaEgQZlqPq2nk4v</td>\n",
       "      <td>['barbadian pop', 'dance pop', 'pop', 'post-te...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>...</td>\n",
       "      <td>816</td>\n",
       "      <td>450</td>\n",
       "      <td>1</td>\n",
       "      <td>5.36</td>\n",
       "      <td>307</td>\n",
       "      <td>30.18</td>\n",
       "      <td>16.64</td>\n",
       "      <td>0.04</td>\n",
       "      <td>11.35</td>\n",
       "      <td>4.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Stressed Out</td>\n",
       "      <td>Twenty One Pilots</td>\n",
       "      <td>3CRDbSIZ4r5MsZ0YwxuEkn</td>\n",
       "      <td>['modern rock', 'rock']</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2015-05-15</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>...</td>\n",
       "      <td>336</td>\n",
       "      <td>525</td>\n",
       "      <td>0</td>\n",
       "      <td>5.29</td>\n",
       "      <td>265</td>\n",
       "      <td>13.47</td>\n",
       "      <td>21.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.63</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Panda</td>\n",
       "      <td>Desiigner</td>\n",
       "      <td>5OOkp4U9P9oL23maHFHL1h</td>\n",
       "      <td>['pop rap', 'rap', 'southern hip hop', 'trap',...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2016-06-26</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>...</td>\n",
       "      <td>600</td>\n",
       "      <td>200</td>\n",
       "      <td>24</td>\n",
       "      <td>5.42</td>\n",
       "      <td>362</td>\n",
       "      <td>18.30</td>\n",
       "      <td>6.10</td>\n",
       "      <td>0.73</td>\n",
       "      <td>11.04</td>\n",
       "      <td>4.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Hello</td>\n",
       "      <td>Adele</td>\n",
       "      <td>0ENSn4fwAbCGeFGVUbXEU3</td>\n",
       "      <td>['british soul', 'pop', 'uk pop']</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-23</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "      <td>...</td>\n",
       "      <td>312</td>\n",
       "      <td>200</td>\n",
       "      <td>9</td>\n",
       "      <td>5.51</td>\n",
       "      <td>142</td>\n",
       "      <td>19.60</td>\n",
       "      <td>12.56</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.92</td>\n",
       "      <td>7.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Dont Let Me Down</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>1i1fxkWeaMmKEB4T7zqbzK</td>\n",
       "      <td>['electropop', 'pop', 'tropical house']</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-02-05</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>...</td>\n",
       "      <td>288</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>5.27</td>\n",
       "      <td>150</td>\n",
       "      <td>18.37</td>\n",
       "      <td>25.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.57</td>\n",
       "      <td>7.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>CANT STOP THE FEELING</td>\n",
       "      <td>Justin Timberlake</td>\n",
       "      <td>6JV2JOEocMgcZxYSZelKcc</td>\n",
       "      <td>['dance pop', 'pop']</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-06</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>...</td>\n",
       "      <td>480</td>\n",
       "      <td>850</td>\n",
       "      <td>0</td>\n",
       "      <td>5.42</td>\n",
       "      <td>252</td>\n",
       "      <td>18.91</td>\n",
       "      <td>33.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.93</td>\n",
       "      <td>6.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Closer</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>7BKLCZ1jbUBVqRi2FVlTVw</td>\n",
       "      <td>['electropop', 'pop', 'tropical house']</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-07-29</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>...</td>\n",
       "      <td>840</td>\n",
       "      <td>425</td>\n",
       "      <td>0</td>\n",
       "      <td>5.03</td>\n",
       "      <td>196</td>\n",
       "      <td>47.46</td>\n",
       "      <td>24.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.07</td>\n",
       "      <td>5.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Title             Artist                      id  \\\n",
       "0          Love Yourself      Justin Bieber  3hB5DgAiMAQ4DzYbsMq1IT   \n",
       "1                  Sorry      Justin Bieber  69bp2EbF7Q2rqc5N3ylezZ   \n",
       "2              One Dance              Drake  1xznGGDReH1oQq0xzbwXa3   \n",
       "3                   Work            Rihanna  14WWzenpaEgQZlqPq2nk4v   \n",
       "4           Stressed Out  Twenty One Pilots  3CRDbSIZ4r5MsZ0YwxuEkn   \n",
       "5                  Panda          Desiigner  5OOkp4U9P9oL23maHFHL1h   \n",
       "6                  Hello              Adele  0ENSn4fwAbCGeFGVUbXEU3   \n",
       "7       Dont Let Me Down   The Chainsmokers  1i1fxkWeaMmKEB4T7zqbzK   \n",
       "8  CANT STOP THE FEELING  Justin Timberlake  6JV2JOEocMgcZxYSZelKcc   \n",
       "9                 Closer   The Chainsmokers  7BKLCZ1jbUBVqRi2FVlTVw   \n",
       "\n",
       "                                       artist_genres  is_single  total_tracks  \\\n",
       "0           ['canadian pop', 'pop', 'post-teen pop']          0            19   \n",
       "1           ['canadian pop', 'pop', 'post-teen pop']          0            19   \n",
       "2  ['canadian hip hop', 'canadian pop', 'hip hop'...          0            20   \n",
       "3  ['barbadian pop', 'dance pop', 'pop', 'post-te...          0            16   \n",
       "4                            ['modern rock', 'rock']          0            14   \n",
       "5  ['pop rap', 'rap', 'southern hip hop', 'trap',...          0            14   \n",
       "6                  ['british soul', 'pop', 'uk pop']          1             1   \n",
       "7            ['electropop', 'pop', 'tropical house']          1             1   \n",
       "8                               ['dance pop', 'pop']          1             1   \n",
       "9            ['electropop', 'pop', 'tropical house']          1             1   \n",
       "\n",
       "  release_date  day_of_week  release_season  Year  ...  words_neg_count  \\\n",
       "0   2015-11-13            5               3  2015  ...              648   \n",
       "1   2015-11-13            5               3  2015  ...             1104   \n",
       "2   2016-05-06            5               1  2016  ...              384   \n",
       "3   2016-01-28            4               4  2016  ...              816   \n",
       "4   2015-05-15            5               1  2015  ...              336   \n",
       "5   2016-06-26            7               2  2016  ...              600   \n",
       "6   2015-10-23            5               3  2015  ...              312   \n",
       "7   2016-02-05            5               4  2016  ...              288   \n",
       "8   2016-05-06            5               1  2016  ...              480   \n",
       "9   2016-07-29            5               2  2016  ...              840   \n",
       "\n",
       "   words_pos_count  words_curse_count  avg_most_common_words_size  \\\n",
       "0              350                  0                        5.08   \n",
       "1              600                  0                        5.34   \n",
       "2              525                  0                        4.98   \n",
       "3              450                  1                        5.36   \n",
       "4              525                  0                        5.29   \n",
       "5              200                 24                        5.42   \n",
       "6              200                  9                        5.51   \n",
       "7              400                  0                        5.27   \n",
       "8              850                  0                        5.42   \n",
       "9              425                  0                        5.03   \n",
       "\n",
       "   words_repeat_count  words_negative_pct  words_positive_pct  \\\n",
       "0                 179               31.50               17.02   \n",
       "1                 117               74.65               40.57   \n",
       "2                 184               21.66               29.61   \n",
       "3                 307               30.18               16.64   \n",
       "4                 265               13.47               21.05   \n",
       "5                 362               18.30                6.10   \n",
       "6                 142               19.60               12.56   \n",
       "7                 150               18.37               25.51   \n",
       "8                 252               18.91               33.49   \n",
       "9                 196               47.46               24.01   \n",
       "\n",
       "   words_curse_pct  words_repeat_pct  stopwords_pct  \n",
       "0             0.00              8.70           7.78  \n",
       "1             0.00              7.91           8.25  \n",
       "2             0.00             10.38           7.28  \n",
       "3             0.04             11.35           4.77  \n",
       "4             0.00             10.63           7.38  \n",
       "5             0.73             11.04           4.79  \n",
       "6             0.57              8.92           7.29  \n",
       "7             0.00              9.57           7.40  \n",
       "8             0.00              9.93           6.70  \n",
       "9             0.00             11.07           5.42  \n",
       "\n",
       "[10 rows x 37 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spotify_add_pct[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify_add_pct.to_csv('./data/spotify_after.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
