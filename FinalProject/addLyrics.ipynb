{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import html\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import time\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (10, 6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition & Data Cleaning & Data Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Load and clean the data\n",
    "The following cells perform 2 things:\n",
    "* load the csv file which contain the dataframe of spotify songs\n",
    "* replace all of the unnecessary punctuation in each title and artist for the further work with scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify=pd.read_csv(\"./data/spotify.csv\")\n",
    "df_spotify=df_spotify.drop('Unnamed: 0', 1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to clean the titles and artists string for the search in metrolyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDf(df):\n",
    "    for i in range(0,len(df)):\n",
    "        df['Title'][i]=df['Title'][i].replace('MotorSport','motor sport').replace('PILLOWTALK','pillow talk').replace('Back To You','Back 2 you')\n",
    "        \n",
    "        list=df['Title'][i].split(' (feat. ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split('- From ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split(' feat. ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split(' (with ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split(' (Original ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split(' (From ')\n",
    "        df['Title'][i]=list[0]\n",
    "        list=df['Title'][i].split(' (Fifty ')\n",
    "        df['Title'][i]=list[0]\n",
    "        \n",
    "        df['Title'][i]=df['Title'][i].replace(\"Wanna\",\"want to\").replace('\\n','').replace(';','').replace(\"'\",\"\").replace(',','').replace('/ ','').replace('- ','').replace('-','').replace('é','e').replace('?','').replace('\"','').replace(\"!\",\"\").replace(\"in'\",\"ing\").replace(\" and \",\" \").replace(\"’\",\"\")\n",
    "        df['Title'][i]=df['Title'][i].replace('(','').replace(')','').replace('.','')\n",
    "        \n",
    "        df['Artist'][i]=df['Artist'][i].replace('P!nk','Pink').replace('NERD','nerd the neptunes').replace('ZAYN','zayn malik').replace('Axwell /\\ Ingrosso','Axwell Ingrosso').replace('Ayo & Teo','Ayo Teo')\n",
    "        df['Artist'][i]=df['Artist'][i].replace('é','e').replace('í','i').replace('.','').replace('\\n','').replace(' + ','-').replace('.','').replace('ó','o').replace('$','s').replace(\"'\",\"\").replace(\"!\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "cleanDf(df_spotify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we create a function to add a column to our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_col(df,lst,colName):\n",
    "    df[colName] = lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the amount of rows in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "587"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_spotify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Load and store lyrics data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to create the lyrics_urls list to store each song's lyrics url on metrolyrics\n",
    "The following function perform 5 things:\n",
    "* create list of urls from metrolyrics for each song \n",
    "* replace all of the unnecessary strings from the title of each song\n",
    "* handle with cases of several artists of a song. split them and take the first one to build with it a url\n",
    "* replace all of the spaces to '-' for the url\n",
    "* clean the text from unnecessary punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_lyrics_urls(df):\n",
    "    #creating the list of urls for each song's lyrics\n",
    "    lyrics_urls=[]\n",
    "    for i in range (0,len(df)):\n",
    "        #the case of & in songs:\n",
    "        name=df['Title'][i].replace(\"& \",\"\").replace(\"'\",\"\")\n",
    "        name=name.replace(\" \",\"-\")\n",
    "\n",
    "        list=df['Artist'][i].split(',')\n",
    "        #we will always take the first artist\n",
    "        artist_str=list[0]\n",
    "        list=artist_str.split(' & ')\n",
    "        artist_str=list[0]\n",
    "        list=artist_str.split(' and ')\n",
    "        artist_str=list[0]\n",
    "        list=artist_str.split('featuring')\n",
    "        artist_str=list[0]\n",
    "\n",
    "        url='https://www.metrolyrics.com/'+name+'-lyrics-'+artist_str.replace('\"','').replace(\" \",\"-\")+'.html'\n",
    "        #the case of \"Maroon-5-.html\"\n",
    "        url=url.replace(\"-.\",\".\") \n",
    "\n",
    "        lyrics_urls.append(url)      \n",
    "    return lyrics_urls    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "587"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=create_lyrics_urls(df_spotify)\n",
    "len(urls) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the nltk package for the further work with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords, movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to delete the stopwords from the lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_mystopwords(string):\n",
    "    tokens = string.split(\" \")\n",
    "    tokens_filtered = [word for word in tokens if not word in stop_words]\n",
    "    return (\" \").join(tokens_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to count the stopwords in each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_stopwords(string):\n",
    "    count=0\n",
    "    tokens = string.split(\" \")\n",
    "    for word in tokens:\n",
    "        if word in stop_words:\n",
    "            count+=1\n",
    "    return count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following cell pefrom 5 things:\n",
    "\n",
    "* scrap the metrolyrics\n",
    "* load the lyrics of each song\n",
    "* check the amount of words in each song \n",
    "* check the amount of stopwords in each song \n",
    "* clean the text from digits, stopwords, and unnecessary punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the part of creation of the lyrics_texts dictionary which contain the lyrics of each song in top 2016,2017,2018\n",
    "\n",
    "all_words_count=[]\n",
    "stopwords_count=[]\n",
    "\n",
    "def find_lyrics_txt(df):\n",
    "    lyrics_texts=[]\n",
    "    time.sleep(7)\n",
    "    for i in range (0,len(df)):\n",
    "        url = urls[i]\n",
    "        response = requests.get(url)\n",
    "        data = response.text\n",
    "        soup = bs(data,'html.parser')\n",
    "        couplets = soup.findAll(\"p\",{\"class\":\"verse\"})\n",
    "        temp = \"\"\n",
    "        for p in couplets:\n",
    "            temp = str(temp) + str(p.text)\n",
    "            temp=temp+\"\\n\"\n",
    "        #cleaning the text from unnecessary punctuation\n",
    "        temp=re.sub(r'[^(a-zA-Z)\\s]','', temp)\n",
    "        temp=temp.replace('[','').replace(']','').replace('(','').replace(')','').replace('\"','').replace(\"'\",\"\")\n",
    "        #remove all of the digits \n",
    "        temp=''.join(j for j in temp if not j.isdigit())\n",
    "        #count the number of words in each song\n",
    "        all_words_count.append(len(temp))\n",
    "        #count the number of stopwords in each song\n",
    "        stopwords_c=count_stopwords(temp)\n",
    "        stopwords_count.append(stopwords_c)\n",
    "        #remove all of the stopwords\n",
    "        temp = remove_mystopwords(temp)\n",
    "        #for the further work with text we would like to lowercase all of the words in text   \n",
    "        lyrics_texts.append(temp.lower())\n",
    "        lyrics_texts[i]=lyrics_texts[i].replace('\\n ','\\n')  \n",
    "    return lyrics_texts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics=find_lyrics_txt(df_spotify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add 2 columns to our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_col(df_spotify,all_words_count,'words_count')\n",
    "append_col(df_spotify,stopwords_count,'stopwords_count')\n",
    "print(stopwords_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_without_empty_strings = []\n",
    "for string in lyrics:\n",
    "    if (string != \"\"):\n",
    "        lyrics_without_empty_strings.append(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we want to check the amount of songs with zero words (the algorithm of finding the url for metrolyrics didn't work for them) so we can delete them in the dataframe in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = [] \n",
    "count=0\n",
    "for i in range(0, len(all_words_count)) : \n",
    "    if all_words_count[i] == 0 : \n",
    "        res_list.append(i)    \n",
    "print(res_list)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning: In the next cell we drop every row which value of amout of words in song is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range (0,len(df_spotify)):\n",
    "    if (df_spotify['words_count'][i]) == 0:\n",
    "        df_spotify['words_count'][i]=pd.np.NaN\n",
    "        \n",
    "df_spotify=df_spotify.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the amount of rows in a new dataframe without nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_spotify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a text as positive, negative or neutral using textblob package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the polarity value for each song and defining the range for each pnn value: \n",
    "\n",
    "* pnn between -0.3 and 0.3 is neutral and its value will marked as 0\n",
    "* pnn above 0.3 is positive and its value will marked as 1\n",
    "* pnn below -0.3 is negative and its value will marked as -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "# The sentiment function of textblob returns two properties, polarity, and subjectivity.\n",
    "# Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement\n",
    "\n",
    "def pos_neg_neutral(lyrics_texts):\n",
    "    blobs=[]\n",
    "    for i in range (0,len(lyrics_texts)):\n",
    "        blob=TextBlob(lyrics_texts[i])\n",
    "        pnn=blob.sentiment.polarity\n",
    "        definition=0\n",
    "        if -0.3 < pnn < 0.3:\n",
    "            definition=0\n",
    "        if pnn  <= -0.3:\n",
    "            definition=-1\n",
    "        if pnn >= 0.3:\n",
    "            definition=1\n",
    "        blobs.append(definition)    \n",
    "    return blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs_polarity=pos_neg_neutral(lyrics_without_empty_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(blobs_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blobs_polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we will check hom many negative,neutral,positive songs do we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs_res={\"pos\": 0, \"neg\": 0, \"neutral\": 0}\n",
    "for i in range (0,len(blobs_polarity)):\n",
    "    if blobs_polarity[i]==1:\n",
    "        blobs_res[\"pos\"]+=1\n",
    "    if blobs_polarity[i]==-1:\n",
    "        blobs_res[\"neg\"]+=1\n",
    "    if blobs_polarity[i]==0:\n",
    "        blobs_res[\"neutral\"]+=1\n",
    "print(blobs_res)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_col(df_spotify,blobs_polarity,'blobs_polarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we will tokenize (convert the whole lyrics text to words) the lyrics of each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lyrics=[]\n",
    "\n",
    "for i in range (0,len(df_spotify)):\n",
    "    tokens=word_tokenize(lyrics_without_empty_strings[i])\n",
    "    tokenized_lyrics.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(tokenized_lyrics[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next 3 cells perfom 3 things:\n",
    "\n",
    "* create a list with positive words by scraping a web page that includes the necessary information for this and count the total amount of these words in each song\n",
    "* create a list with negative words by scraping a web page that includes the necessary information for this and count the total amount of these words in each song\n",
    "* create a list with curse words by scraping a web page that includes the necessary information for this and count the total amount of these words in each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_pos=[]\n",
    "\n",
    "keywords_pos=[]\n",
    "\n",
    "url='https://www.enchantedlearning.com/wordlist/positivewords.shtml'\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "soup = bs(data,'html.parser')\n",
    "words = soup.findAll(\"div\",{\"class\":\"wordlist-section\"})\n",
    "\n",
    "for div in words:\n",
    "    divs=soup.findAll(\"div\",{\"class\":\"wordlist-item\"})\n",
    "    for div in divs:\n",
    "        keywords_pos.append(div.text)\n",
    "\n",
    "for i in range (0,len(df_spotify)):\n",
    "    temp=[]\n",
    "    for word in keywords_pos:\n",
    "        #here we create a temp list to store all the words from keywords_neg which appear in lyrics \n",
    "        temp+=re.findall(word, str(lyrics_without_empty_strings[i]))\n",
    "    #now we'd want to know how many times these words appeared in text\n",
    "    count_pos.append(len(temp))\n",
    "print(count_pos)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_neg=[]\n",
    "\n",
    "keywords_neg=[]\n",
    "\n",
    "url='https://www.enchantedlearning.com/wordlist/negativewords.shtml'\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "soup = bs(data,'html.parser')\n",
    "words = soup.findAll(\"div\",{\"class\":\"wordlist-section\"})\n",
    "\n",
    "for div in words:\n",
    "    divs=soup.findAll(\"div\",{\"class\":\"wordlist-item\"})\n",
    "    for div in divs:\n",
    "        keywords_neg.append(div.text)\n",
    "        \n",
    "for i in range (0,len(df_spotify)):\n",
    "    temp=[]\n",
    "    for word in keywords_neg:\n",
    "        #here we create a temp list to store all the words from keywords_neg which appear in lyrics \n",
    "        temp+=re.findall(word, str(lyrics_without_empty_strings[i]))\n",
    "    #now we'd want to know how many times these words appeared in text\n",
    "    count_neg.append(len(temp))\n",
    "print(count_neg)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_swear=[]\n",
    "\n",
    "keywords_swear=[]\n",
    "\n",
    "url='https://en.wiktionary.org/wiki/Category:English_swear_words'\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "soup = bs(data,'html.parser')\n",
    "div_class = soup.findAll(\"div\",{\"class\":\"mw-category-group\"})\n",
    "\n",
    "for div in div_class:\n",
    "    uls=div.find_all('ul')\n",
    "    for ul in uls:\n",
    "        lis=ul.find_all('li')\n",
    "        for ls in lis:\n",
    "            links=ls.find_all('a')\n",
    "            for link in links:\n",
    "                keywords_swear.append(link.text)\n",
    "                \n",
    "keywords_swear.pop(0)         \n",
    "\n",
    "for k in range(0,len(keywords_swear)):\n",
    "    keywords_swear[k] =keywords_swear[k].lower() \n",
    "\n",
    "# print(keywords_swear)\n",
    "\n",
    "for i in range (0,len(df_spotify)):\n",
    "    temp=[]\n",
    "    for word in keywords_swear:\n",
    "        #here we create a temp list to store all the words from keywords_neg which appear in lyrics \n",
    "        temp+=re.findall(word, str(lyrics_without_empty_strings[i]))\n",
    "    #now we'd want to know how many times these words appeared in text\n",
    "    count_swear.append(len(temp))\n",
    "print(count_swear)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_col(df_spotify,count_neg,'words_neg_count')\n",
    "append_col(df_spotify,count_pos,'words_pos_count')\n",
    "append_col(df_spotify,count_swear,'words_curse_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we'd like to find the total count of repeated words and the most common words in each song using Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "reg = re.compile('\\S{4,}')\n",
    "\n",
    "repeated_words=[]\n",
    "most_common_words=[]\n",
    "\n",
    "for i in range (0,len(df_spotify)):\n",
    "    c = Counter(ma.group() for ma in reg.finditer(lyrics_without_empty_strings[i]))\n",
    "    repeated_words.append(sum(c.values()))\n",
    "    most_common_words.append([k for k,v in c.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(most_common_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_most_common_words_size=[]\n",
    "\n",
    "for i in range(0,len(most_common_words)):\n",
    "    total=0\n",
    "    for word in most_common_words[i]:\n",
    "        total += len(word)\n",
    "    ave_size = round(float(total) / float(len(most_common_words[i])) , 2) \n",
    "    avg_most_common_words_size.append(ave_size)\n",
    "        \n",
    "print(avg_most_common_words_size)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_col(df_spotify,avg_most_common_words_size,'avg_most_common_words_size')\n",
    "append_col(df_spotify,repeated_words,'words_repeat_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify.to_csv('./data/spotify_after.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From this point we saw that we schould add an additional data to our dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify_add_pct = pd.read_csv('./data/spotify_after.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify_add_pct.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next cell we add new columns to the dataframe to see the percentage of:\n",
    "\n",
    "* amount of negative words in the song\n",
    "* amount of positive words in the song\n",
    "* amount of curse words in the song\n",
    "* amount of repeated words in the song\n",
    "* amount of stopwords in the song\n",
    "\n",
    "## to the total number of words in a song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_spotify_add_pct['words_negative_pct'] = round((df_spotify_add_pct['words_neg_count'] / df_spotify_add_pct['words_count'])*100,2)\n",
    "df_spotify_add_pct['words_positive_pct'] = round((df_spotify_add_pct['words_pos_count'] / df_spotify_add_pct['words_count'])*100,2)\n",
    "df_spotify_add_pct['words_curse_pct'] = round((df_spotify_add_pct['words_curse_count'] / df_spotify_add_pct['words_count'])*100,2)\n",
    "df_spotify_add_pct['words_repeat_pct'] = round((df_spotify_add_pct['words_repeat_count'] / df_spotify_add_pct['words_count'])*100,2)\n",
    "df_spotify_add_pct['stopwords_pct'] = round((df_spotify_add_pct['stopwords_count'] / df_spotify_add_pct['words_count'])*100,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify_add_pct[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify_add_pct.to_csv('./data/spotify_after.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
